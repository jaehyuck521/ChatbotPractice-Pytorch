{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a79bae0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    Q  \\\n",
      "0     제 감정이 이상해진 것 같아요. 남편만 보면 화가 치밀어 오르고 감정 조절이 안되요.   \n",
      "1                            더 이상 내 감정을 내가 컨트롤 못 하겠어.   \n",
      "2                         하루종일 오르락내리락 롤러코스터 타는 기분이에요.   \n",
      "3                                   꼭 롤러코스터 타는 것 같아요.   \n",
      "4                          롤러코스터 타는 것처럼 기분이 왔다 갔다 해요.   \n",
      "...                                               ...   \n",
      "5226             한숨 자고 일어나면 괜찮으려나? 해서 자고 일어났는데도 똑같아요.   \n",
      "5227                            자고 나면 괜찮을 줄 알았는데 비슷해…   \n",
      "5228                                 지금도 상태가 계속 안 좋아.   \n",
      "5229                       이게 제 마음과 상관없이 증상이 계속 나타나요.   \n",
      "5230                    나이 들면 고쳐질 줄 알았는데…아직까지 계속 그래요.   \n",
      "\n",
      "                                            A  \n",
      "0             감정이 조절이 안 될 때만큼 힘들 때는 없는 거 같아요.  \n",
      "1                      저도 그 기분 이해해요. 많이 힘드시죠?  \n",
      "2     그럴 때는 밥은 잘 먹었는지, 잠은 잘 잤는지 체크해보는 것도 좋아요.  \n",
      "3                                         NaN  \n",
      "4                                         NaN  \n",
      "...                                       ...  \n",
      "5226                                      NaN  \n",
      "5227                                      NaN  \n",
      "5228                                      NaN  \n",
      "5229                                      NaN  \n",
      "5230                                      NaN  \n",
      "\n",
      "[5231 rows x 2 columns]\n",
      "                              A                        Q\n",
      "0                        반갑습니다.                   좋은 아침.\n",
      "1                     좋은 아침이에요.                   좋은 아침.\n",
      "2                  간밤에 별일 없으셨죠?                   좋은 아침.\n",
      "3                      안녕하시렵니까?                   좋은 아침.\n",
      "4                        안녕하세요!                   좋은 아침.\n",
      "...                         ...                      ...\n",
      "11818        티가 나니까 눈치가 보이는 거죠!           훔쳐보는 것도 눈치 보임.\n",
      "11819             훔쳐보는 거 티나나봐요.           훔쳐보는 것도 눈치 보임.\n",
      "11820                    설렜겠어요.              흑기사 해주는 짝남.\n",
      "11821  잘 헤어질 수 있는 사이 여부인 거 같아요.  힘든 연애 좋은 연애라는게 무슨 차이일까?\n",
      "11822        도피성 결혼은 하지 않길 바라요.               힘들어서 결혼할까봐\n",
      "\n",
      "[13147 rows x 2 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 18378 entries, 0 to 5230\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   A       14181 non-null  object\n",
      " 1   Q       18378 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 430.7+ KB\n",
      "None\n",
      "임의 질문 샘플 정수 인코딩: [3089, 1181, 7705]\n",
      "[7915   17 1496 7705 7916    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "(1, 7917, 256)\n",
      "(1, 7917, 256)\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.7097 - accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.7164 - accuracy: 0.0000e+00\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.7062 - accuracy: 0.0000e+00\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.7155 - accuracy: 0.0000e+00\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.7097 - accuracy: 0.0000e+00\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.7074 - accuracy: 0.0000e+00\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.7080 - accuracy: 0.0000e+00\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.7054 - accuracy: 0.0000e+00\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.7091 - accuracy: 0.0000e+00\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.7110 - accuracy: 0.0000e+00\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.7143 - accuracy: 0.0000e+00\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.6981 - accuracy: 0.0000e+00\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.7089 - accuracy: 0.0000e+00\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.6982 - accuracy: 0.0000e+00\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.7039 - accuracy: 0.0000e+00\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.6999 - accuracy: 0.0000e+00\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.7005 - accuracy: 0.0000e+00\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.7017 - accuracy: 0.0000e+00\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.7069 - accuracy: 0.0000e+00\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.6944 - accuracy: 0.0000e+00\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.7030 - accuracy: 0.0000e+00\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.6977 - accuracy: 0.0000e+00\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6873 - accuracy: 0.0000e+00\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.7051 - accuracy: 0.0000e+00\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.6853 - accuracy: 0.0000e+00\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.6963 - accuracy: 0.0000e+00\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.6899 - accuracy: 0.0000e+00\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.6956 - accuracy: 0.0000e+00\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.6828 - accuracy: 0.0000e+00\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.6900 - accuracy: 0.0000e+00\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.6835 - accuracy: 0.0000e+00\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.6817 - accuracy: 0.0000e+00\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.6804 - accuracy: 0.0000e+00\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.6775 - accuracy: 0.0000e+00\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.6752 - accuracy: 0.0000e+00\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.6672 - accuracy: 0.0000e+00\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.6730 - accuracy: 0.0000e+00\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6718 - accuracy: 0.0000e+00\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.6715 - accuracy: 0.0000e+00\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.6577 - accuracy: 0.0000e+00\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.6629 - accuracy: 0.0000e+00\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.6645 - accuracy: 0.0000e+00\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.6639 - accuracy: 0.0000e+00\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.6485 - accuracy: 0.0000e+00\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.6544 - accuracy: 0.0000e+00\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.6546 - accuracy: 0.0000e+00\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.6475 - accuracy: 0.0000e+00\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.6435 - accuracy: 0.0000e+00\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.6434 - accuracy: 0.0256\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.6334 - accuracy: 0.0256\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.6410 - accuracy: 0.0256\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.6415 - accuracy: 0.0000e+00\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.6341 - accuracy: 0.0256\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.6367 - accuracy: 0.0000e+00\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 79ms/step - loss: 0.6305 - accuracy: 0.0256\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.6289 - accuracy: 0.0256\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.6243 - accuracy: 0.0256\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.6218 - accuracy: 0.0256\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.6202 - accuracy: 0.0256\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.6182 - accuracy: 0.0256\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.6128 - accuracy: 0.0256\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.6173 - accuracy: 0.0000e+00\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.6190 - accuracy: 0.0256\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.6111 - accuracy: 0.0256\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.5995 - accuracy: 0.0256\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.5990 - accuracy: 0.0256\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.5985 - accuracy: 0.0256\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.5971 - accuracy: 0.0256\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.5951 - accuracy: 0.0256\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.5951 - accuracy: 0.0256\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.5933 - accuracy: 0.0256\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5900 - accuracy: 0.0256\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.5859 - accuracy: 0.0256\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.5844 - accuracy: 0.0000e+00\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5846 - accuracy: 0.0256\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.5826 - accuracy: 0.0256\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.5756 - accuracy: 0.0513\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.5714 - accuracy: 0.0256\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5721 - accuracy: 0.0256\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5770 - accuracy: 0.0513\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.5676 - accuracy: 0.0256\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.5693 - accuracy: 0.0513\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.5629 - accuracy: 0.0256\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.5639 - accuracy: 0.0256\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.5539 - accuracy: 0.0769\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.5607 - accuracy: 0.0000e+00\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.5626 - accuracy: 0.0000e+00\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.5526 - accuracy: 0.0256\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.5504 - accuracy: 0.0513\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.5457 - accuracy: 0.0769\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.5515 - accuracy: 0.0256\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.5441 - accuracy: 0.0769\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5453 - accuracy: 0.0256\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5441 - accuracy: 0.0513\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5388 - accuracy: 0.0513\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.5390 - accuracy: 0.0256\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.5364 - accuracy: 0.0769\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.5338 - accuracy: 0.0513\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.5329 - accuracy: 0.0513\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.5291 - accuracy: 0.0513\n",
      "Input: 안녕하세요\n",
      "Output: .\n",
      "Input: 나는 오늘 이별했어\n",
      "Output: .\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import time\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "batch_size=64\n",
    "buffer_size=20000\n",
    "\"\"\"\n",
    "df1=pd.read_excel('1.xlsx')\n",
    "df2=pd.read_excel('3.xlsx')\n",
    "df3=pd.read_excel('6.xlsx')\n",
    "df4=pd.read_excel('11.xlsx')\n",
    "df5=pd.read_excel('12.xlsx')\n",
    "df6=pd.read_excel('13.xlsx')\n",
    "df7=pd.read_excel(\"qa1.xlsx\")\n",
    "# print(df4.columns)\n",
    "# print(df5.columns)\n",
    "# print(df6.columns)\n",
    "df_medium1=pd.concat([df4,df5,df6])\n",
    "# print(df_medium1.info())\n",
    "df_medium2=pd.concat([df1,df2,df3])\n",
    "print(df_medium2.info())\n",
    "df_medium1.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
    "df_medium2.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
    "df_medium1.rename(columns={'question':'Q','answer':'A'},inplace=True)\n",
    "df_medium2.rename(columns={'MQ':'Q','SA':'A'},inplace=True)\n",
    "df_medium3=pd.concat([df_medium1,df_medium2])\n",
    "print(df_medium3.info())\n",
    "df_final=pd.concat([df_medium3,df7])\n",
    "df_final.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
    "\"\"\"\n",
    "df=pd.read_excel('comunications.xlsx')\n",
    "df2=pd.read_csv('ChatbotData.csv')\n",
    "df3=pd.read_excel('웰니스_대화_스크립트_데이터셋.xlsx')\n",
    "df2.drop(['label'], axis=1,inplace=True)\n",
    "df3.drop(['구분'],axis=1, inplace=True)\n",
    "df3.rename(columns = {'유저' : 'Q','챗봇':'A'}, inplace = True)\n",
    "print(df3)\n",
    "df1=pd.concat([df,df2])\n",
    "print(df1)\n",
    "df_final=pd.concat([df1,df3])\n",
    "print(df_final.info())\n",
    "df_final.dropna(axis=0, inplace=True)\n",
    "question=[]\n",
    "answers=[]\n",
    "for sentence in df_final['Q']:\n",
    "    sentence=re.sub(r\"([?.!,])\",r\"\\1 \",sentence)\n",
    "    sentence=sentence.strip()\n",
    "    question.append(sentence)\n",
    "for sentence in df_final['A']:\n",
    "    sentence=re.sub(r\"([?.!,])\",r\"\\1 \",sentence)\n",
    "    sentence=sentence.strip()\n",
    "    answers.append(sentence)\n",
    "\n",
    "    \n",
    "# 정수 인코딩 & 단어 사전 형성\n",
    "tokenizer=tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(question+answers, target_vocab_size=2**13)\n",
    "START_TOKEN, END_TOKEN =[tokenizer.vocab_size],[tokenizer.vocab_size+1]\n",
    "VOCAB_SIZE=tokenizer.vocab_size+2\n",
    "print('임의 질문 샘플 정수 인코딩: {}'.format(tokenizer.encode(question[20])))\n",
    "\n",
    "# 문장의 최대 정수를 정해줌\n",
    "MAX_LENGTH=40\n",
    "def tokenize_and_filter(inputs,outputs):\n",
    "    tokenized_inputs, tokenized_outputs=[],[]\n",
    "    for(sentence1, sentence2) in zip(inputs,outputs):\n",
    "         # 인 코딩 시작, 종료 토큰 추가 \n",
    "        sentence1=START_TOKEN+tokenizer.encode(sentence1)+END_TOKEN\n",
    "        sentence2=END_TOKEN+tokenizer.encode(sentence2)+END_TOKEN\n",
    "        \n",
    "        tokenized_inputs.append(sentence1)\n",
    "        tokenized_outputs.append(sentence2)\n",
    "        \n",
    "        # 패딩\n",
    "        tokenized_inputs=tf.keras.preprocessing.sequence.pad_sequences(tokenized_inputs, maxlen=MAX_LENGTH,padding='post')\n",
    "        tokenized_outputs=tf.keras.preprocessing.sequence.pad_sequences(tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "        \n",
    "        return tokenized_inputs, tokenized_outputs\n",
    "    \n",
    "question,answers=tokenize_and_filter(question, answers)\n",
    "print(question[0])\n",
    "\n",
    "dataset=tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs':question,\n",
    "        'dec_inputs':answers[:,:-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs':answers[:,1:]\n",
    "    }\n",
    "\n",
    "))\n",
    "\n",
    "dataset=dataset.cache()\n",
    "dataset=dataset.shuffle(buffer_size)\n",
    "dataset=dataset.batch(batch_size)\n",
    "dataset=dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# 트랜스 포머 모델\n",
    "def transformer(vocab_size, num_layers, dff,\n",
    "                d_model, num_heads, dropout,\n",
    "                name=\"transformer\"):\n",
    "\n",
    "  # 인코더의 입력\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "  # 디코더의 입력\n",
    "  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "  # 인코더의 패딩 마스크\n",
    "  enc_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='enc_padding_mask')(inputs)\n",
    "\n",
    "  # 디코더의 룩어헤드 마스크(첫번째 서브층)\n",
    "  look_ahead_mask = tf.keras.layers.Lambda(\n",
    "      create_look_ahead_mask, output_shape=(1, None, None),\n",
    "      name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "  # 디코더의 패딩 마스크(두번째 서브층)\n",
    "  dec_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='dec_padding_mask')(inputs)\n",
    "\n",
    "  # 인코더의 출력은 enc_outputs. 디코더로 전달된다.\n",
    "  enc_outputs = encoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,\n",
    "      d_model=d_model, num_heads=num_heads, dropout=dropout,\n",
    "  )(inputs=[inputs, enc_padding_mask]) # 인코더의 입력은 입력 문장과 패딩 마스크\n",
    "\n",
    "  # 디코더의 출력은 dec_outputs. 출력층으로 전달된다.\n",
    "  dec_outputs = decoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,\n",
    "      d_model=d_model, num_heads=num_heads, dropout=dropout,\n",
    "  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "  # 다음 단어 예측을 위한 출력층\n",
    "  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding=self.positional_encoding(position, d_model)\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles=1/tf.pow(10000,(2*(i//2))/tf.cast(d_model,tf.float32))\n",
    "        return position * angles\n",
    "    def positional_encoding(self,position, d_model):\n",
    "        angle_rads=self.get_angles(position=tf.range(position, dtype=tf.float32)[:,tf.newaxis],i=tf.range(d_model, dtype=tf.float32)[tf.newaxis,:],d_model=d_model)\n",
    "        # 짝수에는 사인 함수 적용\n",
    "        sines=tf.math.sin(angle_rads[:,0::2])\n",
    "        # 홀수에는 코사인 함수 적용\n",
    "        cosines=tf.math.cos(angle_rads[:,1::2])\n",
    "        \n",
    "        angle_rads=np.zeros(angle_rads.shape)\n",
    "        angle_rads[:,0::2]=sines\n",
    "        angle_rads[:,1::2]=cosines\n",
    "        \n",
    "        pos_encoding=tf.constant(angle_rads)\n",
    "        pos_encoding=pos_encoding[tf.newaxis,...]\n",
    "        print(pos_encoding.shape)\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "    def call(self, inputs):\n",
    "        return inputs+self.pos_encoding[:,:tf.shape(inputs)[1],:]\n",
    " \n",
    "def create_padding_mask(x):\n",
    "    mask=tf.cast(tf.math.equal(x,0),tf.float32)\n",
    "    return mask[:,tf.newaxis,tf.newaxis,:]\n",
    "\n",
    "# 디코더의 첫번째 서브층(sublayer)에서 미래 토큰을 Mask하는 함수\n",
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x) # 패딩 마스크도 포함\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)\n",
    "\n",
    "#encoder\n",
    "def encoder(vocab_size, num_layers, dff,\n",
    "                d_model, num_heads, dropout,\n",
    "                name=\"encoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "  # 인코더는 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # 포지셔널 인코딩 + 드롭아웃\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  # 인코더를 num_layers개 쌓기\n",
    "    for i in range(num_layers):\n",
    "        outputs = encoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\n",
    "            dropout=dropout, name=\"encoder_layer_{}\".format(i),\n",
    "        )([outputs, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, padding_mask], outputs=outputs, name=name)     \n",
    "\n",
    "def encoder_layer(dff, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "      # 인코더는 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "      # 멀티-헤드 어텐션 (첫번째 서브층 / 셀프 어텐션)\n",
    "    attention = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention\")({\n",
    "            'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V\n",
    "            'mask': padding_mask # 패딩 마스크 사용\n",
    "            })\n",
    "\n",
    "      # 드롭아웃 + 잔차 연결과 층 정규화\n",
    "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "    attention = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "      # 포지션 와이즈 피드 포워드 신경망 (두번째 서브층)\n",
    "    outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "      # 드롭아웃 + 잔차 연결과 층 정규화\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "    # d_model을 num_heads로 나눈 값.\n",
    "    # 논문 기준 : 64\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "    # WQ, WK, WV에 해당하는 밀집층 정의\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    # WO에 해당하는 밀집층 정의\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "  # num_heads 개수만큼 q, k, v를 split하는 함수\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(\n",
    "            inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "        'value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "    # 1. WQ, WK, WV에 해당하는 밀집층 지나기\n",
    "    # q : (batch_size, query의 문장 길이, d_model)\n",
    "    # k : (batch_size, key의 문장 길이, d_model)\n",
    "    # v : (batch_size, value의 문장 길이, d_model)\n",
    "    # 참고) 인코더(k, v)-디코더(q) 어텐션에서는 query 길이와 key, value의 길이는 다를 수 있다.\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "    # 2. 헤드 나누기\n",
    "    # q : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "    # k : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n",
    "    # v : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "    # 3. 스케일드 닷 프로덕트 어텐션. 앞서 구현한 함수 사용.\n",
    "    # (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "        scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask)\n",
    "    # (batch_size, query의 문장 길이, num_heads, d_model/num_heads)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "    # 4. 헤드 연결(concatenate)하기\n",
    "    # (batch_size, query의 문장 길이, d_model)\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))\n",
    "\n",
    "    # 5. WO에 해당하는 밀집층 지나기\n",
    "    # (batch_size, query의 문장 길이, d_model)\n",
    "        outputs = self.dense(concat_attention)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "  # query 크기 : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "  # key 크기 : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n",
    "  # value 크기 : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n",
    "  # padding_mask : (batch_size, 1, 1, key의 문장 길이)\n",
    "\n",
    "  # Q와 K의 곱. 어텐션 스코어 행렬.\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "  # 스케일링\n",
    "  # dk의 루트값으로 나눠준다.\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "  # 마스킹. 어텐션 스코어 행렬의 마스킹 할 위치에 매우 작은 음수값을 넣는다.\n",
    "  # 매우 작은 값이므로 소프트맥스 함수를 지나면 행렬의 해당 위치의 값은 0이 된다.\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "\n",
    "  # 소프트맥스 함수는 마지막 차원인 key의 문장 길이 방향으로 수행된다.\n",
    "  # attention weight : (batch_size, num_heads, query의 문장 길이, key의 문장 길이)\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "  # output : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "def decoder(vocab_size, num_layers, dff,\n",
    "            d_model, num_heads, dropout,\n",
    "            name='decoder'):\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "\n",
    "  # 디코더는 룩어헤드 마스크(첫번째 서브층)와 패딩 마스크(두번째 서브층) 둘 다 사용.\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name='look_ahead_mask')\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "  # 포지셔널 인코딩 + 드롭아웃\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  # 디코더를 num_layers개 쌓기\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\n",
    "            dropout=dropout, name='decoder_layer_{}'.format(i),\n",
    "        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)\n",
    "\n",
    "def decoder_layer(dff, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "\n",
    "  # 룩어헤드 마스크(첫번째 서브층)\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "        shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "\n",
    "  # 패딩 마스크(두번째 서브층)\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "  # 멀티-헤드 어텐션 (첫번째 서브층 / 마스크드 셀프 어텐션)\n",
    "    attention1 = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V\n",
    "          'mask': look_ahead_mask # 룩어헤드 마스크\n",
    "        })\n",
    "\n",
    "  # 잔차 연결과 층 정규화\n",
    "    attention1 = tf.keras.layers.LayerNormalization(\n",
    "          epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "  # 멀티-헤드 어텐션 (두번째 서브층 / 디코더-인코더 어텐션)\n",
    "    attention2 = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "            'query': attention1, 'key': enc_outputs, 'value': enc_outputs, # Q != K = V\n",
    "            'mask': padding_mask # 패딩 마스크\n",
    "        })\n",
    "\n",
    "  # 드롭아웃 + 잔차 연결과 층 정규화\n",
    "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "    attention2 = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "  # 포지션 와이즈 피드 포워드 신경망 (세번째 서브층)\n",
    "    outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention2)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 드롭아웃 + 잔차 연결과 층 정규화\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "D_MODEL = 256\n",
    "NUM_LAYERS = 2\n",
    "NUM_HEADS = 8\n",
    "DFF = 512\n",
    "DROPOUT = 0.1\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dff=DFF,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "            arg1 = tf.math.rsqrt(step)\n",
    "            arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "            return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "learning_rate=CustomSchedule(D_MODEL)\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "def accuracy(y_true,y_pred):\n",
    "    y_true=tf.reshape(y_true, shape=(-1,MAX_LENGTH-1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
    "EPOCHS=100\n",
    "model.fit(dataset,epochs=EPOCHS)\n",
    "\n",
    "\n",
    "def evaluate(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "    output = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "  # 디코더의 예측 시작\n",
    "    for i in range(MAX_LENGTH):\n",
    "        predictions = model(inputs=[sentence, output], training=False)\n",
    "\n",
    "    # 현재(마지막) 시점의 예측 단어를 받아온다.\n",
    "        predictions = predictions[:, -1:, :]\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # 만약 마지막 시점의 예측 단어가 종료 토큰이라면 예측을 중단\n",
    "        if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "             break\n",
    "\n",
    "    # 마지막 시점의 예측 단어를 출력에 연결한다.\n",
    "    # 이는 for문을 통해서 디코더의 입력으로 사용될 예정이다.\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "        return tf.squeeze(output, axis=0)\n",
    "\n",
    "def predict(sentence):\n",
    "    prediction = evaluate(sentence)\n",
    "\n",
    "    predicted_sentence = tokenizer.decode([i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Output: {}'.format(predicted_sentence))\n",
    "\n",
    "    return predicted_sentence\n",
    "  \n",
    "  \n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence\n",
    "\n",
    "output=predict(\"안녕하세요\")\n",
    "output=predict(\"나는 오늘 이별했어\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c306bcc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a857dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356a0ceb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
