{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5a79bae0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 14181 entries, 0 to 5215\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   A       14181 non-null  object\n",
      " 1   Q       14181 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 332.4+ KB\n",
      "None\n",
      "                                     A                       Q\n",
      "0                           많이 힘든가봅니다.                     한숨만\n",
      "1      정해져 있지는 않지만 길지 않은게 서로에게 좋을 거예요.                몇일정도 썸타?\n",
      "2                        내 집 마련 축하드려요.              새 집 가기로 했어\n",
      "3                적극적이면 오히려 더 좋을 것 같아요.  좋아하는 사람한테 적극적인 여자 별로야?\n",
      "4                   혼자 준비하기는 조금 벅찰텐대요.                셀프웨딩 해볼까\n",
      "...                                ...                     ...\n",
      "14176     저도 그럴 때가 많아요. 지금도 긴장하고 있는걸요?         물론 많이 떨기는 하지만요.\n",
      "14177                           흔들리나요.               보고 싶다고 하네\n",
      "14178                 이별의 이유가 있을 테니까요.          다시 한번 붙잡아도 안되네\n",
      "14179                            공개해요.         남자친구한테 핸드폰 공개해?\n",
      "14180             오래된 건 오래된 대로 매력이 있죠.               오래된 게 좋겠지\n",
      "\n",
      "[14181 rows x 2 columns]\n",
      "임의 질문 샘플 정수 인코딩: [781, 4811, 788]\n",
      "[7915 6262 7916    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "(1, 7917, 256)\n",
      "(1, 7917, 256)\n",
      "Epoch 1/500\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.9293 - accuracy: 0.0000e+00\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.9204 - accuracy: 0.0000e+00\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.9212 - accuracy: 0.0000e+00\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.9344 - accuracy: 0.0000e+00\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.9157 - accuracy: 0.0000e+00\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.9243 - accuracy: 0.0000e+00\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.9224 - accuracy: 0.0000e+00\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.9213 - accuracy: 0.0000e+00\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.9211 - accuracy: 0.0000e+00\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.9311 - accuracy: 0.0000e+00\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.9168 - accuracy: 0.0000e+00\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.9169 - accuracy: 0.0000e+00\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.9218 - accuracy: 0.0000e+00\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.9169 - accuracy: 0.0000e+00\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.9160 - accuracy: 0.0000e+00\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.9119 - accuracy: 0.0000e+00\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.9073 - accuracy: 0.0000e+00\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.9143 - accuracy: 0.0000e+00\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.9144 - accuracy: 0.0000e+00\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.9023 - accuracy: 0.0000e+00\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.9132 - accuracy: 0.0000e+00\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.9043 - accuracy: 0.0000e+00\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.9093 - accuracy: 0.0000e+00\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.8933 - accuracy: 0.0000e+00\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.8978 - accuracy: 0.0000e+00\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.8913 - accuracy: 0.0000e+00\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.8967 - accuracy: 0.0000e+00\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.9152 - accuracy: 0.0000e+00\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.8824 - accuracy: 0.0000e+00\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.8810 - accuracy: 0.0000e+00\n",
      "Epoch 31/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.9051 - accuracy: 0.0000e+00\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.8869 - accuracy: 0.0000e+00\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.8837 - accuracy: 0.0000e+00\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.8931 - accuracy: 0.0000e+00\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.8813 - accuracy: 0.0000e+00\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.8830 - accuracy: 0.0000e+00\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.8939 - accuracy: 0.0000e+00\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.8692 - accuracy: 0.0000e+00\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.8792 - accuracy: 0.0000e+00\n",
      "Epoch 40/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.8827 - accuracy: 0.0000e+00\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.8659 - accuracy: 0.0000e+00\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.8724 - accuracy: 0.0000e+00\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.8573 - accuracy: 0.0000e+00\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.8621 - accuracy: 0.0000e+00\n",
      "Epoch 45/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.8649 - accuracy: 0.0000e+00\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.8608 - accuracy: 0.0000e+00\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.8501 - accuracy: 0.0000e+00\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.8541 - accuracy: 0.0000e+00\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.8552 - accuracy: 0.0000e+00\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.8555 - accuracy: 0.0000e+00\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.8413 - accuracy: 0.0256\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.8512 - accuracy: 0.0000e+00\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.8306 - accuracy: 0.0256\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.8278 - accuracy: 0.0513\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.8424 - accuracy: 0.0000e+00\n",
      "Epoch 56/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.8297 - accuracy: 0.0256\n",
      "Epoch 57/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.8238 - accuracy: 0.0513\n",
      "Epoch 58/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.8237 - accuracy: 0.0256\n",
      "Epoch 59/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.8232 - accuracy: 0.0000e+00\n",
      "Epoch 60/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.8166 - accuracy: 0.0769\n",
      "Epoch 61/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.8185 - accuracy: 0.0256\n",
      "Epoch 62/500\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.8108 - accuracy: 0.0513\n",
      "Epoch 63/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.8059 - accuracy: 0.0256\n",
      "Epoch 64/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.8020 - accuracy: 0.1026\n",
      "Epoch 65/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.8043 - accuracy: 0.0513\n",
      "Epoch 66/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.7941 - accuracy: 0.0513\n",
      "Epoch 67/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.7944 - accuracy: 0.0769\n",
      "Epoch 68/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 71ms/step - loss: 0.7952 - accuracy: 0.0256\n",
      "Epoch 69/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.7874 - accuracy: 0.0513\n",
      "Epoch 70/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.7983 - accuracy: 0.0256\n",
      "Epoch 71/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.8010 - accuracy: 0.0513\n",
      "Epoch 72/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.7884 - accuracy: 0.0256\n",
      "Epoch 73/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.7794 - accuracy: 0.0256\n",
      "Epoch 74/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.7869 - accuracy: 0.0769\n",
      "Epoch 75/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.7758 - accuracy: 0.0769\n",
      "Epoch 76/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.7713 - accuracy: 0.0256\n",
      "Epoch 77/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.7678 - accuracy: 0.0769\n",
      "Epoch 78/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.7682 - accuracy: 0.0769\n",
      "Epoch 79/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.7648 - accuracy: 0.0256\n",
      "Epoch 80/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.7718 - accuracy: 0.0513\n",
      "Epoch 81/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.7683 - accuracy: 0.0513\n",
      "Epoch 82/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.7628 - accuracy: 0.0256\n",
      "Epoch 83/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.7628 - accuracy: 0.0769\n",
      "Epoch 84/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.7527 - accuracy: 0.0769\n",
      "Epoch 85/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.7593 - accuracy: 0.0513\n",
      "Epoch 86/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.7591 - accuracy: 0.0513\n",
      "Epoch 87/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.7557 - accuracy: 0.0256\n",
      "Epoch 88/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.7423 - accuracy: 0.1026\n",
      "Epoch 89/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.7511 - accuracy: 0.0256\n",
      "Epoch 90/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.7447 - accuracy: 0.0256\n",
      "Epoch 91/500\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.7400 - accuracy: 0.0256\n",
      "Epoch 92/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.7333 - accuracy: 0.0513\n",
      "Epoch 93/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.7350 - accuracy: 0.0513\n",
      "Epoch 94/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.7351 - accuracy: 0.0513\n",
      "Epoch 95/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.7439 - accuracy: 0.0256\n",
      "Epoch 96/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.7321 - accuracy: 0.0256\n",
      "Epoch 97/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.7400 - accuracy: 0.0256\n",
      "Epoch 98/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.7285 - accuracy: 0.0256\n",
      "Epoch 99/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.7329 - accuracy: 0.0513\n",
      "Epoch 100/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.7157 - accuracy: 0.1026\n",
      "Epoch 101/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.7263 - accuracy: 0.0256\n",
      "Epoch 102/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.7253 - accuracy: 0.0513\n",
      "Epoch 103/500\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.7183 - accuracy: 0.0513\n",
      "Epoch 104/500\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.7178 - accuracy: 0.0513\n",
      "Epoch 105/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.7128 - accuracy: 0.1026\n",
      "Epoch 106/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.7193 - accuracy: 0.0769\n",
      "Epoch 107/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.7087 - accuracy: 0.1026\n",
      "Epoch 108/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.7152 - accuracy: 0.0513\n",
      "Epoch 109/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.7141 - accuracy: 0.0769\n",
      "Epoch 110/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.7038 - accuracy: 0.0769\n",
      "Epoch 111/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.7058 - accuracy: 0.0769\n",
      "Epoch 112/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.7040 - accuracy: 0.1026\n",
      "Epoch 113/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.7061 - accuracy: 0.0769\n",
      "Epoch 114/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.6924 - accuracy: 0.1026\n",
      "Epoch 115/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6918 - accuracy: 0.0769\n",
      "Epoch 116/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6984 - accuracy: 0.0256\n",
      "Epoch 117/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6892 - accuracy: 0.1026\n",
      "Epoch 118/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6901 - accuracy: 0.0513\n",
      "Epoch 119/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6828 - accuracy: 0.1026\n",
      "Epoch 120/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6852 - accuracy: 0.1026\n",
      "Epoch 121/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6880 - accuracy: 0.0769\n",
      "Epoch 122/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6723 - accuracy: 0.1026\n",
      "Epoch 123/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6805 - accuracy: 0.0769\n",
      "Epoch 124/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.6860 - accuracy: 0.0769\n",
      "Epoch 125/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6742 - accuracy: 0.1026\n",
      "Epoch 126/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6764 - accuracy: 0.0513\n",
      "Epoch 127/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.6714 - accuracy: 0.1026\n",
      "Epoch 128/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6762 - accuracy: 0.0769\n",
      "Epoch 129/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6719 - accuracy: 0.1026\n",
      "Epoch 130/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6770 - accuracy: 0.0769\n",
      "Epoch 131/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6669 - accuracy: 0.1026\n",
      "Epoch 132/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6639 - accuracy: 0.0769\n",
      "Epoch 133/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6644 - accuracy: 0.1026\n",
      "Epoch 134/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6612 - accuracy: 0.1026\n",
      "Epoch 135/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6599 - accuracy: 0.1026\n",
      "Epoch 136/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6540 - accuracy: 0.1026\n",
      "Epoch 137/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6578 - accuracy: 0.1026\n",
      "Epoch 138/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6546 - accuracy: 0.0769\n",
      "Epoch 139/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6487 - accuracy: 0.1026\n",
      "Epoch 140/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.6517 - accuracy: 0.1026\n",
      "Epoch 141/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6445 - accuracy: 0.1026\n",
      "Epoch 142/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6468 - accuracy: 0.1026\n",
      "Epoch 143/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6377 - accuracy: 0.1026\n",
      "Epoch 144/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6382 - accuracy: 0.1026\n",
      "Epoch 145/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6399 - accuracy: 0.1026\n",
      "Epoch 146/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6353 - accuracy: 0.1026\n",
      "Epoch 147/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6267 - accuracy: 0.1026\n",
      "Epoch 148/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6382 - accuracy: 0.0769\n",
      "Epoch 149/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6234 - accuracy: 0.1026\n",
      "Epoch 150/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 71ms/step - loss: 0.6202 - accuracy: 0.1026\n",
      "Epoch 151/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6285 - accuracy: 0.1026\n",
      "Epoch 152/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.6187 - accuracy: 0.1026\n",
      "Epoch 153/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6241 - accuracy: 0.1026\n",
      "Epoch 154/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6196 - accuracy: 0.1026\n",
      "Epoch 155/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6168 - accuracy: 0.1026\n",
      "Epoch 156/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6039 - accuracy: 0.1026\n",
      "Epoch 157/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6028 - accuracy: 0.1026\n",
      "Epoch 158/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6125 - accuracy: 0.1026\n",
      "Epoch 159/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6098 - accuracy: 0.1026\n",
      "Epoch 160/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5991 - accuracy: 0.1026\n",
      "Epoch 161/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5993 - accuracy: 0.1026\n",
      "Epoch 162/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.5881 - accuracy: 0.1026\n",
      "Epoch 163/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.5976 - accuracy: 0.1026\n",
      "Epoch 164/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5835 - accuracy: 0.1026\n",
      "Epoch 165/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.5828 - accuracy: 0.1026\n",
      "Epoch 166/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.5819 - accuracy: 0.1026\n",
      "Epoch 167/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5865 - accuracy: 0.1026\n",
      "Epoch 168/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5907 - accuracy: 0.1026\n",
      "Epoch 169/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5769 - accuracy: 0.1026\n",
      "Epoch 170/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5754 - accuracy: 0.1026\n",
      "Epoch 171/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5771 - accuracy: 0.1026\n",
      "Epoch 172/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5685 - accuracy: 0.1026\n",
      "Epoch 173/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5642 - accuracy: 0.1026\n",
      "Epoch 174/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5716 - accuracy: 0.1026\n",
      "Epoch 175/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5733 - accuracy: 0.1026\n",
      "Epoch 176/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5588 - accuracy: 0.1026\n",
      "Epoch 177/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5580 - accuracy: 0.1026\n",
      "Epoch 178/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.5536 - accuracy: 0.1026\n",
      "Epoch 179/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5537 - accuracy: 0.1026\n",
      "Epoch 180/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5450 - accuracy: 0.1026\n",
      "Epoch 181/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5430 - accuracy: 0.1026\n",
      "Epoch 182/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5371 - accuracy: 0.1026\n",
      "Epoch 183/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.5387 - accuracy: 0.1026\n",
      "Epoch 184/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5329 - accuracy: 0.1026\n",
      "Epoch 185/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.5292 - accuracy: 0.1026\n",
      "Epoch 186/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5327 - accuracy: 0.1026\n",
      "Epoch 187/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.5299 - accuracy: 0.1026\n",
      "Epoch 188/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5253 - accuracy: 0.1026\n",
      "Epoch 189/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.5204 - accuracy: 0.1026\n",
      "Epoch 190/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5201 - accuracy: 0.1026\n",
      "Epoch 191/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.5215 - accuracy: 0.1026\n",
      "Epoch 192/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5141 - accuracy: 0.1026\n",
      "Epoch 193/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5116 - accuracy: 0.1026\n",
      "Epoch 194/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5049 - accuracy: 0.1026\n",
      "Epoch 195/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5120 - accuracy: 0.1026\n",
      "Epoch 196/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5128 - accuracy: 0.1026\n",
      "Epoch 197/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4981 - accuracy: 0.1026\n",
      "Epoch 198/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4977 - accuracy: 0.1026\n",
      "Epoch 199/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4954 - accuracy: 0.1026\n",
      "Epoch 200/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4959 - accuracy: 0.1026\n",
      "Epoch 201/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4936 - accuracy: 0.1026\n",
      "Epoch 202/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.4864 - accuracy: 0.1026\n",
      "Epoch 203/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4841 - accuracy: 0.1026\n",
      "Epoch 204/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4830 - accuracy: 0.1026\n",
      "Epoch 205/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4759 - accuracy: 0.1026\n",
      "Epoch 206/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4816 - accuracy: 0.1026\n",
      "Epoch 207/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4741 - accuracy: 0.1026\n",
      "Epoch 208/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4696 - accuracy: 0.1026\n",
      "Epoch 209/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4703 - accuracy: 0.1026\n",
      "Epoch 210/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4696 - accuracy: 0.1026\n",
      "Epoch 211/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.4698 - accuracy: 0.1026\n",
      "Epoch 212/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4619 - accuracy: 0.1026\n",
      "Epoch 213/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4552 - accuracy: 0.1026\n",
      "Epoch 214/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4512 - accuracy: 0.1026\n",
      "Epoch 215/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4440 - accuracy: 0.1026\n",
      "Epoch 216/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4511 - accuracy: 0.1026\n",
      "Epoch 217/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4432 - accuracy: 0.1026\n",
      "Epoch 218/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4392 - accuracy: 0.1026\n",
      "Epoch 219/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4350 - accuracy: 0.1026\n",
      "Epoch 220/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4289 - accuracy: 0.1026\n",
      "Epoch 221/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4388 - accuracy: 0.1026\n",
      "Epoch 222/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.4303 - accuracy: 0.1026\n",
      "Epoch 223/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4271 - accuracy: 0.1026\n",
      "Epoch 224/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4234 - accuracy: 0.1026\n",
      "Epoch 225/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4195 - accuracy: 0.1026\n",
      "Epoch 226/500\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.4212 - accuracy: 0.1026\n",
      "Epoch 227/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4209 - accuracy: 0.1026\n",
      "Epoch 228/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.4196 - accuracy: 0.1026\n",
      "Epoch 229/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4076 - accuracy: 0.1026\n",
      "Epoch 230/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4084 - accuracy: 0.1026\n",
      "Epoch 231/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4035 - accuracy: 0.1026\n",
      "Epoch 232/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3954 - accuracy: 0.1026\n",
      "Epoch 233/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4037 - accuracy: 0.1026\n",
      "Epoch 234/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3941 - accuracy: 0.1026\n",
      "Epoch 235/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3942 - accuracy: 0.1026\n",
      "Epoch 236/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3874 - accuracy: 0.1026\n",
      "Epoch 237/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3859 - accuracy: 0.1026\n",
      "Epoch 238/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.3831 - accuracy: 0.1026\n",
      "Epoch 239/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3873 - accuracy: 0.1026\n",
      "Epoch 240/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3867 - accuracy: 0.1026\n",
      "Epoch 241/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3748 - accuracy: 0.1026\n",
      "Epoch 242/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3762 - accuracy: 0.1026\n",
      "Epoch 243/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3764 - accuracy: 0.1026\n",
      "Epoch 244/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3694 - accuracy: 0.1026\n",
      "Epoch 245/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3661 - accuracy: 0.1026\n",
      "Epoch 246/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3637 - accuracy: 0.1026\n",
      "Epoch 247/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3731 - accuracy: 0.1026\n",
      "Epoch 248/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3564 - accuracy: 0.1026\n",
      "Epoch 249/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3566 - accuracy: 0.1026\n",
      "Epoch 250/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3481 - accuracy: 0.1026\n",
      "Epoch 251/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3491 - accuracy: 0.1026\n",
      "Epoch 252/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3493 - accuracy: 0.1026\n",
      "Epoch 253/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3576 - accuracy: 0.1026\n",
      "Epoch 254/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.3397 - accuracy: 0.1026\n",
      "Epoch 255/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3359 - accuracy: 0.1026\n",
      "Epoch 256/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3333 - accuracy: 0.1026\n",
      "Epoch 257/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3339 - accuracy: 0.1026\n",
      "Epoch 258/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3293 - accuracy: 0.1026\n",
      "Epoch 259/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3296 - accuracy: 0.1026\n",
      "Epoch 260/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3254 - accuracy: 0.1026\n",
      "Epoch 261/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3209 - accuracy: 0.1026\n",
      "Epoch 262/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3145 - accuracy: 0.1026\n",
      "Epoch 263/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3152 - accuracy: 0.1026\n",
      "Epoch 264/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3151 - accuracy: 0.1026\n",
      "Epoch 265/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3147 - accuracy: 0.1026\n",
      "Epoch 266/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.3022 - accuracy: 0.1026\n",
      "Epoch 267/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.3019 - accuracy: 0.1026\n",
      "Epoch 268/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3012 - accuracy: 0.1026\n",
      "Epoch 269/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3019 - accuracy: 0.1026\n",
      "Epoch 270/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.2921 - accuracy: 0.1026\n",
      "Epoch 271/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2885 - accuracy: 0.1026\n",
      "Epoch 272/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.2879 - accuracy: 0.1026\n",
      "Epoch 273/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.2917 - accuracy: 0.1026\n",
      "Epoch 274/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2883 - accuracy: 0.1026\n",
      "Epoch 275/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.2805 - accuracy: 0.1026\n",
      "Epoch 276/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2789 - accuracy: 0.1026\n",
      "Epoch 277/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2757 - accuracy: 0.1026\n",
      "Epoch 278/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2733 - accuracy: 0.1026\n",
      "Epoch 279/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.2696 - accuracy: 0.1026\n",
      "Epoch 280/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2678 - accuracy: 0.1026\n",
      "Epoch 281/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.2659 - accuracy: 0.1026\n",
      "Epoch 282/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2622 - accuracy: 0.1026\n",
      "Epoch 283/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2580 - accuracy: 0.1026\n",
      "Epoch 284/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2547 - accuracy: 0.1026\n",
      "Epoch 285/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2562 - accuracy: 0.1026\n",
      "Epoch 286/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2478 - accuracy: 0.1026\n",
      "Epoch 287/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2490 - accuracy: 0.1026\n",
      "Epoch 288/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.2411 - accuracy: 0.1026\n",
      "Epoch 289/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.2424 - accuracy: 0.1026\n",
      "Epoch 290/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2394 - accuracy: 0.1026\n",
      "Epoch 291/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2295 - accuracy: 0.1026\n",
      "Epoch 292/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.2341 - accuracy: 0.1026\n",
      "Epoch 293/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.2310 - accuracy: 0.1026\n",
      "Epoch 294/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2306 - accuracy: 0.1026\n",
      "Epoch 295/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.2269 - accuracy: 0.1026\n",
      "Epoch 296/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2202 - accuracy: 0.1026\n",
      "Epoch 297/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2185 - accuracy: 0.1026\n",
      "Epoch 298/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2163 - accuracy: 0.1026\n",
      "Epoch 299/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2120 - accuracy: 0.1026\n",
      "Epoch 300/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.2127 - accuracy: 0.1026\n",
      "Epoch 301/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.2050 - accuracy: 0.1026\n",
      "Epoch 302/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2018 - accuracy: 0.1026\n",
      "Epoch 303/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2006 - accuracy: 0.1026\n",
      "Epoch 304/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.2012 - accuracy: 0.1026\n",
      "Epoch 305/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.1962 - accuracy: 0.1026\n",
      "Epoch 306/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.1916 - accuracy: 0.1026\n",
      "Epoch 307/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.1892 - accuracy: 0.1026\n",
      "Epoch 308/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.1866 - accuracy: 0.1026\n",
      "Epoch 309/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.1835 - accuracy: 0.1026\n",
      "Epoch 310/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.1813 - accuracy: 0.1026\n",
      "Epoch 311/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.1796 - accuracy: 0.1026\n",
      "Epoch 312/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.1790 - accuracy: 0.1026\n",
      "Epoch 313/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.1741 - accuracy: 0.1026\n",
      "Epoch 314/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 65ms/step - loss: 0.1743 - accuracy: 0.1026\n",
      "Epoch 315/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.1690 - accuracy: 0.1026\n",
      "Epoch 316/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.1654 - accuracy: 0.1026\n",
      "Epoch 317/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.1665 - accuracy: 0.1026\n",
      "Epoch 318/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.1612 - accuracy: 0.1026\n",
      "Epoch 319/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.1585 - accuracy: 0.1026\n",
      "Epoch 320/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.1601 - accuracy: 0.1026\n",
      "Epoch 321/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.1522 - accuracy: 0.1026\n",
      "Epoch 322/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.1531 - accuracy: 0.1026\n",
      "Epoch 323/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.1504 - accuracy: 0.1026\n",
      "Epoch 324/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.1494 - accuracy: 0.1026\n",
      "Epoch 325/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.1459 - accuracy: 0.1026\n",
      "Epoch 326/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.1427 - accuracy: 0.1026\n",
      "Epoch 327/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.1408 - accuracy: 0.1026\n",
      "Epoch 328/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.1405 - accuracy: 0.1026\n",
      "Epoch 329/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.1335 - accuracy: 0.1026\n",
      "Epoch 330/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.1309 - accuracy: 0.1026\n",
      "Epoch 331/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.1386 - accuracy: 0.1026\n",
      "Epoch 332/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.1285 - accuracy: 0.1026\n",
      "Epoch 333/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.1263 - accuracy: 0.1026\n",
      "Epoch 334/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.1210 - accuracy: 0.1026\n",
      "Epoch 335/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.1237 - accuracy: 0.1026\n",
      "Epoch 336/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.1190 - accuracy: 0.1026\n",
      "Epoch 337/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.1188 - accuracy: 0.1026\n",
      "Epoch 338/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.1141 - accuracy: 0.1026\n",
      "Epoch 339/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.1129 - accuracy: 0.1026\n",
      "Epoch 340/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.1120 - accuracy: 0.1026\n",
      "Epoch 341/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.1087 - accuracy: 0.1026\n",
      "Epoch 342/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.1070 - accuracy: 0.1026\n",
      "Epoch 343/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.1045 - accuracy: 0.1026\n",
      "Epoch 344/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.1045 - accuracy: 0.1026\n",
      "Epoch 345/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.1020 - accuracy: 0.1026\n",
      "Epoch 346/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.1010 - accuracy: 0.1026\n",
      "Epoch 347/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0964 - accuracy: 0.1026\n",
      "Epoch 348/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0945 - accuracy: 0.1026\n",
      "Epoch 349/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0928 - accuracy: 0.1026\n",
      "Epoch 350/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0921 - accuracy: 0.1026\n",
      "Epoch 351/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0902 - accuracy: 0.1026\n",
      "Epoch 352/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0888 - accuracy: 0.1026\n",
      "Epoch 353/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0895 - accuracy: 0.1026\n",
      "Epoch 354/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0853 - accuracy: 0.1026\n",
      "Epoch 355/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0830 - accuracy: 0.1026\n",
      "Epoch 356/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0802 - accuracy: 0.1026\n",
      "Epoch 357/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0797 - accuracy: 0.1026\n",
      "Epoch 358/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0796 - accuracy: 0.1026\n",
      "Epoch 359/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0788 - accuracy: 0.1026\n",
      "Epoch 360/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0768 - accuracy: 0.1026\n",
      "Epoch 361/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0747 - accuracy: 0.1026\n",
      "Epoch 362/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0727 - accuracy: 0.1026\n",
      "Epoch 363/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0688 - accuracy: 0.1026\n",
      "Epoch 364/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0710 - accuracy: 0.1026\n",
      "Epoch 365/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0695 - accuracy: 0.1026\n",
      "Epoch 366/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0678 - accuracy: 0.1026\n",
      "Epoch 367/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0646 - accuracy: 0.1026\n",
      "Epoch 368/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0642 - accuracy: 0.1026\n",
      "Epoch 369/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0635 - accuracy: 0.1026\n",
      "Epoch 370/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0619 - accuracy: 0.1026\n",
      "Epoch 371/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0605 - accuracy: 0.1026\n",
      "Epoch 372/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0591 - accuracy: 0.1026\n",
      "Epoch 373/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0599 - accuracy: 0.1026\n",
      "Epoch 374/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0574 - accuracy: 0.1026\n",
      "Epoch 375/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0571 - accuracy: 0.1026\n",
      "Epoch 376/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0561 - accuracy: 0.1026\n",
      "Epoch 377/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0527 - accuracy: 0.1026\n",
      "Epoch 378/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0533 - accuracy: 0.1026\n",
      "Epoch 379/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0533 - accuracy: 0.1026\n",
      "Epoch 380/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0519 - accuracy: 0.1026\n",
      "Epoch 381/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0501 - accuracy: 0.1026\n",
      "Epoch 382/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0500 - accuracy: 0.1026\n",
      "Epoch 383/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0485 - accuracy: 0.1026\n",
      "Epoch 384/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0476 - accuracy: 0.1026\n",
      "Epoch 385/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0452 - accuracy: 0.1026\n",
      "Epoch 386/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0447 - accuracy: 0.1026\n",
      "Epoch 387/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0450 - accuracy: 0.1026\n",
      "Epoch 388/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0433 - accuracy: 0.1026\n",
      "Epoch 389/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0429 - accuracy: 0.1026\n",
      "Epoch 390/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0434 - accuracy: 0.1026\n",
      "Epoch 391/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0397 - accuracy: 0.1026\n",
      "Epoch 392/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0403 - accuracy: 0.1026\n",
      "Epoch 393/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0398 - accuracy: 0.1026\n",
      "Epoch 394/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0383 - accuracy: 0.1026\n",
      "Epoch 395/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0388 - accuracy: 0.1026\n",
      "Epoch 396/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0381 - accuracy: 0.1026\n",
      "Epoch 397/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0368 - accuracy: 0.1026\n",
      "Epoch 398/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0362 - accuracy: 0.1026\n",
      "Epoch 399/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0354 - accuracy: 0.1026\n",
      "Epoch 400/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0349 - accuracy: 0.1026\n",
      "Epoch 401/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0352 - accuracy: 0.1026\n",
      "Epoch 402/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0344 - accuracy: 0.1026\n",
      "Epoch 403/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0342 - accuracy: 0.1026\n",
      "Epoch 404/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0336 - accuracy: 0.1026\n",
      "Epoch 405/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0327 - accuracy: 0.1026\n",
      "Epoch 406/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0314 - accuracy: 0.1026\n",
      "Epoch 407/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0308 - accuracy: 0.1026\n",
      "Epoch 408/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0310 - accuracy: 0.1026\n",
      "Epoch 409/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0293 - accuracy: 0.1026\n",
      "Epoch 410/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0285 - accuracy: 0.1026\n",
      "Epoch 411/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0292 - accuracy: 0.1026\n",
      "Epoch 412/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0283 - accuracy: 0.1026\n",
      "Epoch 413/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0280 - accuracy: 0.1026\n",
      "Epoch 414/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0281 - accuracy: 0.1026\n",
      "Epoch 415/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0271 - accuracy: 0.1026\n",
      "Epoch 416/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0272 - accuracy: 0.1026\n",
      "Epoch 417/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0262 - accuracy: 0.1026\n",
      "Epoch 418/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0252 - accuracy: 0.1026\n",
      "Epoch 419/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0255 - accuracy: 0.1026\n",
      "Epoch 420/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0247 - accuracy: 0.1026\n",
      "Epoch 421/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0255 - accuracy: 0.1026\n",
      "Epoch 422/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0245 - accuracy: 0.1026\n",
      "Epoch 423/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0234 - accuracy: 0.1026\n",
      "Epoch 424/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0240 - accuracy: 0.1026\n",
      "Epoch 425/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0222 - accuracy: 0.1026\n",
      "Epoch 426/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0221 - accuracy: 0.1026\n",
      "Epoch 427/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0221 - accuracy: 0.1026\n",
      "Epoch 428/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0213 - accuracy: 0.1026\n",
      "Epoch 429/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0210 - accuracy: 0.1026\n",
      "Epoch 430/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0206 - accuracy: 0.1026\n",
      "Epoch 431/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0209 - accuracy: 0.1026\n",
      "Epoch 432/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0209 - accuracy: 0.1026\n",
      "Epoch 433/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0204 - accuracy: 0.1026\n",
      "Epoch 434/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0198 - accuracy: 0.1026\n",
      "Epoch 435/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0198 - accuracy: 0.1026\n",
      "Epoch 436/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0194 - accuracy: 0.1026\n",
      "Epoch 437/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0189 - accuracy: 0.1026\n",
      "Epoch 438/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0181 - accuracy: 0.1026\n",
      "Epoch 439/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0176 - accuracy: 0.1026\n",
      "Epoch 440/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0176 - accuracy: 0.1026\n",
      "Epoch 441/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0177 - accuracy: 0.1026\n",
      "Epoch 442/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0183 - accuracy: 0.1026\n",
      "Epoch 443/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0168 - accuracy: 0.1026\n",
      "Epoch 444/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0166 - accuracy: 0.1026\n",
      "Epoch 445/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0162 - accuracy: 0.1026\n",
      "Epoch 446/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0161 - accuracy: 0.1026\n",
      "Epoch 447/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0158 - accuracy: 0.1026\n",
      "Epoch 448/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0154 - accuracy: 0.1026\n",
      "Epoch 449/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0158 - accuracy: 0.1026\n",
      "Epoch 450/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0156 - accuracy: 0.1026\n",
      "Epoch 451/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0154 - accuracy: 0.1026\n",
      "Epoch 452/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0150 - accuracy: 0.1026\n",
      "Epoch 453/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0145 - accuracy: 0.1026\n",
      "Epoch 454/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0141 - accuracy: 0.1026\n",
      "Epoch 455/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0141 - accuracy: 0.1026\n",
      "Epoch 456/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0139 - accuracy: 0.1026\n",
      "Epoch 457/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0136 - accuracy: 0.1026\n",
      "Epoch 458/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0133 - accuracy: 0.1026\n",
      "Epoch 459/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0136 - accuracy: 0.1026\n",
      "Epoch 460/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0133 - accuracy: 0.1026\n",
      "Epoch 461/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0131 - accuracy: 0.1026\n",
      "Epoch 462/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0127 - accuracy: 0.1026\n",
      "Epoch 463/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0128 - accuracy: 0.1026\n",
      "Epoch 464/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0122 - accuracy: 0.1026\n",
      "Epoch 465/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0124 - accuracy: 0.1026\n",
      "Epoch 466/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0116 - accuracy: 0.1026\n",
      "Epoch 467/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0114 - accuracy: 0.1026\n",
      "Epoch 468/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0114 - accuracy: 0.1026\n",
      "Epoch 469/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0122 - accuracy: 0.1026\n",
      "Epoch 470/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0122 - accuracy: 0.1026\n",
      "Epoch 471/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0114 - accuracy: 0.1026\n",
      "Epoch 472/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0111 - accuracy: 0.1026\n",
      "Epoch 473/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0110 - accuracy: 0.1026\n",
      "Epoch 474/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0106 - accuracy: 0.1026\n",
      "Epoch 475/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0104 - accuracy: 0.1026\n",
      "Epoch 476/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0106 - accuracy: 0.1026\n",
      "Epoch 477/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0099 - accuracy: 0.1026\n",
      "Epoch 478/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0105 - accuracy: 0.1026\n",
      "Epoch 479/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0101 - accuracy: 0.1026\n",
      "Epoch 480/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0098 - accuracy: 0.1026\n",
      "Epoch 481/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0099 - accuracy: 0.1026\n",
      "Epoch 482/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0097 - accuracy: 0.1026\n",
      "Epoch 483/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0097 - accuracy: 0.1026\n",
      "Epoch 484/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0092 - accuracy: 0.1026\n",
      "Epoch 485/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0092 - accuracy: 0.1026\n",
      "Epoch 486/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0094 - accuracy: 0.1026\n",
      "Epoch 487/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0091 - accuracy: 0.1026\n",
      "Epoch 488/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0089 - accuracy: 0.1026\n",
      "Epoch 489/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0088 - accuracy: 0.1026\n",
      "Epoch 490/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0086 - accuracy: 0.1026\n",
      "Epoch 491/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0083 - accuracy: 0.1026\n",
      "Epoch 492/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0089 - accuracy: 0.1026\n",
      "Epoch 493/500\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0083 - accuracy: 0.1026\n",
      "Epoch 494/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0078 - accuracy: 0.1026\n",
      "Epoch 495/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0079 - accuracy: 0.1026\n",
      "Epoch 496/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0082 - accuracy: 0.1026\n",
      "Epoch 497/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0079 - accuracy: 0.1026\n",
      "Epoch 498/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0077 - accuracy: 0.1026\n",
      "Epoch 499/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0077 - accuracy: 0.1026\n",
      "Epoch 500/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0077 - accuracy: 0.1026\n",
      "Input: 오늘 기분이 좀 좋다...\n",
      "Output: 힘든가봅니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import time\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "batch_size=64\n",
    "buffer_size=20000\n",
    "\"\"\"\n",
    "df1=pd.read_excel('1.xlsx')\n",
    "df2=pd.read_excel('3.xlsx')\n",
    "df3=pd.read_excel('6.xlsx')\n",
    "df4=pd.read_excel('11.xlsx')\n",
    "df5=pd.read_excel('12.xlsx')\n",
    "df6=pd.read_excel('13.xlsx')\n",
    "df7=pd.read_excel(\"qa1.xlsx\")\n",
    "# print(df4.columns)\n",
    "# print(df5.columns)\n",
    "# print(df6.columns)\n",
    "df_medium1=pd.concat([df4,df5,df6])\n",
    "# print(df_medium1.info())\n",
    "df_medium2=pd.concat([df1,df2,df3])\n",
    "print(df_medium2.info())\n",
    "df_medium1.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
    "df_medium2.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
    "df_medium1.rename(columns={'question':'Q','answer':'A'},inplace=True)\n",
    "df_medium2.rename(columns={'MQ':'Q','SA':'A'},inplace=True)\n",
    "df_medium3=pd.concat([df_medium1,df_medium2])\n",
    "print(df_medium3.info())\n",
    "df_final=pd.concat([df_medium3,df7])\n",
    "df_final.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
    "\"\"\"\n",
    "df=pd.read_excel('comunications.xlsx')\n",
    "df2=pd.read_csv('ChatbotData.csv')\n",
    "df3=pd.read_excel('웰니스_대화_스크립트_데이터셋.xlsx')\n",
    "df2.drop(['label'], axis=1,inplace=True)\n",
    "df3.drop(['구분'],axis=1, inplace=True)\n",
    "df3.rename(columns = {'유저' : 'Q','챗봇':'A'}, inplace = True)\n",
    "\n",
    "df1=pd.concat([df,df2])\n",
    "\n",
    "df_final=pd.concat([df1,df3])\n",
    "df_final.dropna(axis=0, inplace=True)\n",
    "print(df_final.info())\n",
    "\n",
    "df_final=df_final.sample(frac=1).reset_index(drop=True)\n",
    "print(df_final)\n",
    "question=[]\n",
    "answers=[]\n",
    "for sentence in df_final['Q']:\n",
    "    sentence=re.sub(r\"([?.!,])\",r\"\\1 \",sentence)\n",
    "    sentence=sentence.strip()\n",
    "    question.append(sentence)\n",
    "for sentence in df_final['A']:\n",
    "    sentence=re.sub(r\"([?.!,])\",r\"\\1 \",sentence)\n",
    "    sentence=sentence.strip()\n",
    "    answers.append(sentence)\n",
    "\n",
    "    \n",
    "# 정수 인코딩 & 단어 사전 형성\n",
    "tokenizer=tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(question+answers, target_vocab_size=2**13)\n",
    "START_TOKEN, END_TOKEN =[tokenizer.vocab_size],[tokenizer.vocab_size+1]\n",
    "VOCAB_SIZE=tokenizer.vocab_size+2\n",
    "print('임의 질문 샘플 정수 인코딩: {}'.format(tokenizer.encode(question[20])))\n",
    "\n",
    "# 문장의 최대 정수를 정해줌\n",
    "MAX_LENGTH=40\n",
    "def tokenize_and_filter(inputs,outputs):\n",
    "    tokenized_inputs, tokenized_outputs=[],[]\n",
    "    for(sentence1, sentence2) in zip(inputs,outputs):\n",
    "         # 인 코딩 시작, 종료 토큰 추가 \n",
    "        sentence1=START_TOKEN+tokenizer.encode(sentence1)+END_TOKEN\n",
    "        sentence2=END_TOKEN+tokenizer.encode(sentence2)+END_TOKEN\n",
    "        \n",
    "        tokenized_inputs.append(sentence1)\n",
    "        tokenized_outputs.append(sentence2)\n",
    "        \n",
    "        # 패딩\n",
    "        tokenized_inputs=tf.keras.preprocessing.sequence.pad_sequences(tokenized_inputs, maxlen=MAX_LENGTH,padding='post')\n",
    "        tokenized_outputs=tf.keras.preprocessing.sequence.pad_sequences(tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "        \n",
    "        return tokenized_inputs, tokenized_outputs\n",
    "    \n",
    "question,answers=tokenize_and_filter(question, answers)\n",
    "print(question[0])\n",
    "\n",
    "dataset=tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs':question,\n",
    "        'dec_inputs':answers[:,:-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs':answers[:,1:]\n",
    "    },\n",
    "\n",
    "))\n",
    "\n",
    "dataset=dataset.cache()\n",
    "dataset=dataset.shuffle(buffer_size)\n",
    "dataset=dataset.batch(batch_size)\n",
    "dataset=dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# 트랜스 포머 모델\n",
    "def transformer(vocab_size, num_layers, dff,\n",
    "                d_model, num_heads, dropout,\n",
    "                name=\"transformer\"):\n",
    "\n",
    "  # 인코더의 입력\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "  # 디코더의 입력\n",
    "  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "  # 인코더의 패딩 마스크\n",
    "  enc_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='enc_padding_mask')(inputs)\n",
    "\n",
    "  # 디코더의 룩어헤드 마스크(첫번째 서브층)\n",
    "  look_ahead_mask = tf.keras.layers.Lambda(\n",
    "      create_look_ahead_mask, output_shape=(1, None, None),\n",
    "      name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "  # 디코더의 패딩 마스크(두번째 서브층)\n",
    "  dec_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='dec_padding_mask')(inputs)\n",
    "\n",
    "  # 인코더의 출력은 enc_outputs. 디코더로 전달된다.\n",
    "  enc_outputs = encoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,\n",
    "      d_model=d_model, num_heads=num_heads, dropout=dropout,\n",
    "  )(inputs=[inputs, enc_padding_mask]) # 인코더의 입력은 입력 문장과 패딩 마스크\n",
    "\n",
    "  # 디코더의 출력은 dec_outputs. 출력층으로 전달된다.\n",
    "  dec_outputs = decoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,\n",
    "      d_model=d_model, num_heads=num_heads, dropout=dropout,\n",
    "  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "  # 다음 단어 예측을 위한 출력층\n",
    "  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding=self.positional_encoding(position, d_model)\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles=1/tf.pow(10000,(2*(i//2))/tf.cast(d_model,tf.float32))\n",
    "        return position * angles\n",
    "    def positional_encoding(self,position, d_model):\n",
    "        angle_rads=self.get_angles(position=tf.range(position, dtype=tf.float32)[:,tf.newaxis],i=tf.range(d_model, dtype=tf.float32)[tf.newaxis,:],d_model=d_model)\n",
    "        # 짝수에는 사인 함수 적용\n",
    "        sines=tf.math.sin(angle_rads[:,0::2])\n",
    "        # 홀수에는 코사인 함수 적용\n",
    "        cosines=tf.math.cos(angle_rads[:,1::2])\n",
    "        \n",
    "        angle_rads=np.zeros(angle_rads.shape)\n",
    "        angle_rads[:,0::2]=sines\n",
    "        angle_rads[:,1::2]=cosines\n",
    "        \n",
    "        pos_encoding=tf.constant(angle_rads)\n",
    "        pos_encoding=pos_encoding[tf.newaxis,...]\n",
    "        print(pos_encoding.shape)\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "    def call(self, inputs):\n",
    "        return inputs+self.pos_encoding[:,:tf.shape(inputs)[1],:]\n",
    " \n",
    "def create_padding_mask(x):\n",
    "    mask=tf.cast(tf.math.equal(x,0),tf.float32)\n",
    "    return mask[:,tf.newaxis,tf.newaxis,:]\n",
    "\n",
    "# 디코더의 첫번째 서브층(sublayer)에서 미래 토큰을 Mask하는 함수\n",
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x) # 패딩 마스크도 포함\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)\n",
    "\n",
    "#encoder\n",
    "def encoder(vocab_size, num_layers, dff,\n",
    "                d_model, num_heads, dropout,\n",
    "                name=\"encoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "  # 인코더는 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # 포지셔널 인코딩 + 드롭아웃\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  # 인코더를 num_layers개 쌓기\n",
    "    for i in range(num_layers):\n",
    "        outputs = encoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\n",
    "            dropout=dropout, name=\"encoder_layer_{}\".format(i),\n",
    "        )([outputs, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, padding_mask], outputs=outputs, name=name)     \n",
    "\n",
    "def encoder_layer(dff, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "      # 인코더는 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "      # 멀티-헤드 어텐션 (첫번째 서브층 / 셀프 어텐션)\n",
    "    attention = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention\")({\n",
    "            'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V\n",
    "            'mask': padding_mask # 패딩 마스크 사용\n",
    "            })\n",
    "\n",
    "      # 드롭아웃 + 잔차 연결과 층 정규화\n",
    "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "    attention = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "      # 포지션 와이즈 피드 포워드 신경망 (두번째 서브층)\n",
    "    outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "      # 드롭아웃 + 잔차 연결과 층 정규화\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "    # d_model을 num_heads로 나눈 값.\n",
    "    # 논문 기준 : 64\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "    # WQ, WK, WV에 해당하는 밀집층 정의\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    # WO에 해당하는 밀집층 정의\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "  # num_heads 개수만큼 q, k, v를 split하는 함수\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(\n",
    "            inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "        'value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "    # 1. WQ, WK, WV에 해당하는 밀집층 지나기\n",
    "    # q : (batch_size, query의 문장 길이, d_model)\n",
    "    # k : (batch_size, key의 문장 길이, d_model)\n",
    "    # v : (batch_size, value의 문장 길이, d_model)\n",
    "    # 참고) 인코더(k, v)-디코더(q) 어텐션에서는 query 길이와 key, value의 길이는 다를 수 있다.\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "    # 2. 헤드 나누기\n",
    "    # q : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "    # k : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n",
    "    # v : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "    # 3. 스케일드 닷 프로덕트 어텐션. 앞서 구현한 함수 사용.\n",
    "    # (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "        scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask)\n",
    "    # (batch_size, query의 문장 길이, num_heads, d_model/num_heads)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "    # 4. 헤드 연결(concatenate)하기\n",
    "    # (batch_size, query의 문장 길이, d_model)\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))\n",
    "\n",
    "    # 5. WO에 해당하는 밀집층 지나기\n",
    "    # (batch_size, query의 문장 길이, d_model)\n",
    "        outputs = self.dense(concat_attention)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "  # query 크기 : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "  # key 크기 : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n",
    "  # value 크기 : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n",
    "  # padding_mask : (batch_size, 1, 1, key의 문장 길이)\n",
    "\n",
    "  # Q와 K의 곱. 어텐션 스코어 행렬.\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "  # 스케일링\n",
    "  # dk의 루트값으로 나눠준다.\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "  # 마스킹. 어텐션 스코어 행렬의 마스킹 할 위치에 매우 작은 음수값을 넣는다.\n",
    "  # 매우 작은 값이므로 소프트맥스 함수를 지나면 행렬의 해당 위치의 값은 0이 된다.\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "\n",
    "  # 소프트맥스 함수는 마지막 차원인 key의 문장 길이 방향으로 수행된다.\n",
    "  # attention weight : (batch_size, num_heads, query의 문장 길이, key의 문장 길이)\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "  # output : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "def decoder(vocab_size, num_layers, dff,\n",
    "            d_model, num_heads, dropout,\n",
    "            name='decoder'):\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "\n",
    "  # 디코더는 룩어헤드 마스크(첫번째 서브층)와 패딩 마스크(두번째 서브층) 둘 다 사용.\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name='look_ahead_mask')\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "  # 포지셔널 인코딩 + 드롭아웃\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  # 디코더를 num_layers개 쌓기\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\n",
    "            dropout=dropout, name='decoder_layer_{}'.format(i),\n",
    "        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)\n",
    "\n",
    "def decoder_layer(dff, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "\n",
    "  # 룩어헤드 마스크(첫번째 서브층)\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "        shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "\n",
    "  # 패딩 마스크(두번째 서브층)\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "  # 멀티-헤드 어텐션 (첫번째 서브층 / 마스크드 셀프 어텐션)\n",
    "    attention1 = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V\n",
    "          'mask': look_ahead_mask # 룩어헤드 마스크\n",
    "        })\n",
    "\n",
    "  # 잔차 연결과 층 정규화\n",
    "    attention1 = tf.keras.layers.LayerNormalization(\n",
    "          epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "  # 멀티-헤드 어텐션 (두번째 서브층 / 디코더-인코더 어텐션)\n",
    "    attention2 = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "            'query': attention1, 'key': enc_outputs, 'value': enc_outputs, # Q != K = V\n",
    "            'mask': padding_mask # 패딩 마스크\n",
    "        })\n",
    "\n",
    "  # 드롭아웃 + 잔차 연결과 층 정규화\n",
    "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "    attention2 = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "  # 포지션 와이즈 피드 포워드 신경망 (세번째 서브층)\n",
    "    outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention2)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 드롭아웃 + 잔차 연결과 층 정규화\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "D_MODEL = 256\n",
    "NUM_LAYERS = 2\n",
    "NUM_HEADS = 8\n",
    "DFF = 512\n",
    "DROPOUT = 0.1\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dff=DFF,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "            arg1 = tf.math.rsqrt(step)\n",
    "            arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "            return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "learning_rate=CustomSchedule(D_MODEL)\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "def accuracy(y_true,y_pred):\n",
    "    y_true=tf.reshape(y_true, shape=(-1,MAX_LENGTH-1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
    "\n",
    "EPOCHS=500\n",
    "model.fit(dataset,epochs=EPOCHS)\n",
    "\n",
    "\n",
    "def evaluate(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "    output = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "  # 디코더의 예측 시작\n",
    "    for i in range(MAX_LENGTH):\n",
    "        predictions = model(inputs=[sentence, output], training=False)\n",
    "\n",
    "    # 현재(마지막) 시점의 예측 단어를 받아온다.\n",
    "        predictions = predictions[:, -1:, :]\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # 만약 마지막 시점의 예측 단어가 종료 토큰이라면 예측을 중단\n",
    "        if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "             break\n",
    "\n",
    "    # 마지막 시점의 예측 단어를 출력에 연결한다.\n",
    "    # 이는 for문을 통해서 디코더의 입력으로 사용될 예정이다.\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0)\n",
    "\n",
    "def predict(sentence):\n",
    "    prediction = evaluate(sentence)\n",
    "\n",
    "    predicted_sentence = tokenizer.decode(\n",
    "        [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Output: {}'.format(predicted_sentence))\n",
    "\n",
    "    return predicted_sentence\n",
    "  \n",
    "  \n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence\n",
    "\n",
    "\n",
    "output=predict(\"오늘 기분이 좀 좋다...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c306bcc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a857dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356a0ceb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
