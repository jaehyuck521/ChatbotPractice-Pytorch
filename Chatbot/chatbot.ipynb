{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a79bae0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 14181 entries, 0 to 5215\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   A       14181 non-null  object\n",
      " 1   Q       14181 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 332.4+ KB\n",
      "None\n",
      "                                                   A  \\\n",
      "0                               그렇다면 당신의 역할은 다 한거예요.   \n",
      "1                              발전이 없다고 너무 두려워하지 마세요.   \n",
      "2                                          명상을 해보세요.   \n",
      "3                            여기서 끝내기엔 당신은 소중한 사람입니다.   \n",
      "4                                    마음은 식혀지지 않았나봐요.   \n",
      "...                                              ...   \n",
      "14176                                      잘 하실 거예요!   \n",
      "14177                               선택할 때 힘든 건 당연해요.   \n",
      "14178  저를 만들어 준 사람을 부모님, 저랑 이야기해 주는 사람을 친구로 생각하고 있어요   \n",
      "14179                                         신고하세요.   \n",
      "14180                                 마음이 따뜻할 것 같아요.   \n",
      "\n",
      "                               Q  \n",
      "0                진짜 마지막까지 믿었습니다.  \n",
      "1                    제자리걸음만 하는 듯  \n",
      "2                    마음의 평화가 필요해  \n",
      "3                  이 세상 하직할뻔 했네.  \n",
      "4      머리 식히러 간 여행에서 되려 힘듦이 찾아오네  \n",
      "...                          ...  \n",
      "14176                       두고 봐  \n",
      "14177              선택 장애 있는 거 같아  \n",
      "14178                   가족관계 알려줘  \n",
      "14179                  보이스피싱 당했어  \n",
      "14180            강아지 좋아하는 여자 어때?  \n",
      "\n",
      "[14181 rows x 2 columns]\n",
      "임의 질문 샘플 정수 인코딩: [1230, 1091, 1480, 1881, 392, 7722]\n",
      "[7915  102 3628  550 5793  202 7705 7916    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "(1, 7917, 256)\n",
      "(1, 7917, 256)\n",
      "Epoch 1/500\n",
      "1/1 [==============================] - 13s 13s/step - loss: 2.0587 - accuracy: 0.0000e+00\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.0615 - accuracy: 0.0000e+00\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.0522 - accuracy: 0.0000e+00\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.0559 - accuracy: 0.0000e+00\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.0725 - accuracy: 0.0000e+00\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.0632 - accuracy: 0.0000e+00\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.0606 - accuracy: 0.0000e+00\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.0462 - accuracy: 0.0000e+00\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.0531 - accuracy: 0.0000e+00\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 2.0603 - accuracy: 0.0000e+00\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.0544 - accuracy: 0.0000e+00\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.0494 - accuracy: 0.0000e+00\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.0323 - accuracy: 0.0000e+00\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 2.0430 - accuracy: 0.0000e+00\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.0519 - accuracy: 0.0000e+00\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.0552 - accuracy: 0.0000e+00\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.0505 - accuracy: 0.0000e+00\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.0281 - accuracy: 0.0000e+00\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.0302 - accuracy: 0.0000e+00\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.0301 - accuracy: 0.0000e+00\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 2.0259 - accuracy: 0.0000e+00\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 2.0190 - accuracy: 0.0000e+00\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.0216 - accuracy: 0.0000e+00\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 2.0185 - accuracy: 0.0000e+00\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.0137 - accuracy: 0.0000e+00\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.0067 - accuracy: 0.0000e+00\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.0147 - accuracy: 0.0000e+00\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.0089 - accuracy: 0.0000e+00\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 2.0055 - accuracy: 0.0000e+00\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.0114 - accuracy: 0.0000e+00\n",
      "Epoch 31/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.9847 - accuracy: 0.0000e+00\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 1.9920 - accuracy: 0.0000e+00\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 1.9879 - accuracy: 0.0000e+00\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.9935 - accuracy: 0.0000e+00\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.9834 - accuracy: 0.0000e+00\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.9820 - accuracy: 0.0000e+00\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 1.9700 - accuracy: 0.0000e+00\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.9818 - accuracy: 0.0000e+00\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.9763 - accuracy: 0.0000e+00\n",
      "Epoch 40/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.9653 - accuracy: 0.0000e+00\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.9579 - accuracy: 0.0000e+00\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 1.9645 - accuracy: 0.0000e+00\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.9543 - accuracy: 0.0000e+00\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.9555 - accuracy: 0.0000e+00\n",
      "Epoch 45/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.9471 - accuracy: 0.0000e+00\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.9394 - accuracy: 0.0000e+00\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.9403 - accuracy: 0.0000e+00\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.9283 - accuracy: 0.0000e+00\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.9300 - accuracy: 0.0000e+00\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.9216 - accuracy: 0.0000e+00\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 1.9200 - accuracy: 0.0256\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.9224 - accuracy: 0.0000e+00\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.9038 - accuracy: 0.0000e+00\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.9114 - accuracy: 0.0000e+00\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.9000 - accuracy: 0.0256\n",
      "Epoch 56/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.9019 - accuracy: 0.0256\n",
      "Epoch 57/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.8956 - accuracy: 0.0000e+00\n",
      "Epoch 58/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.9000 - accuracy: 0.0000e+00\n",
      "Epoch 59/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.8915 - accuracy: 0.0256\n",
      "Epoch 60/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.8775 - accuracy: 0.0513\n",
      "Epoch 61/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.8801 - accuracy: 0.0256\n",
      "Epoch 62/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.8732 - accuracy: 0.0513\n",
      "Epoch 63/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.8822 - accuracy: 0.0256\n",
      "Epoch 64/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 71ms/step - loss: 1.8644 - accuracy: 0.0513\n",
      "Epoch 65/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.8585 - accuracy: 0.0513\n",
      "Epoch 66/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.8596 - accuracy: 0.0256\n",
      "Epoch 67/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.8615 - accuracy: 0.0256\n",
      "Epoch 68/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.8497 - accuracy: 0.0256\n",
      "Epoch 69/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.8393 - accuracy: 0.0769\n",
      "Epoch 70/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.8337 - accuracy: 0.0769\n",
      "Epoch 71/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.8345 - accuracy: 0.0513\n",
      "Epoch 72/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.8350 - accuracy: 0.0256\n",
      "Epoch 73/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.8282 - accuracy: 0.0513\n",
      "Epoch 74/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.8204 - accuracy: 0.0513\n",
      "Epoch 75/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.8212 - accuracy: 0.0513\n",
      "Epoch 76/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.8213 - accuracy: 0.0513\n",
      "Epoch 77/500\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 1.8174 - accuracy: 0.0256\n",
      "Epoch 78/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.8185 - accuracy: 0.0256\n",
      "Epoch 79/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.8163 - accuracy: 0.0256\n",
      "Epoch 80/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.8023 - accuracy: 0.0256\n",
      "Epoch 81/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.7960 - accuracy: 0.0769\n",
      "Epoch 82/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.7941 - accuracy: 0.0769\n",
      "Epoch 83/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.7968 - accuracy: 0.0513\n",
      "Epoch 84/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.7844 - accuracy: 0.0769\n",
      "Epoch 85/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 1.7834 - accuracy: 0.0769\n",
      "Epoch 86/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.7943 - accuracy: 0.0256\n",
      "Epoch 87/500\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 1.7776 - accuracy: 0.0513\n",
      "Epoch 88/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.7709 - accuracy: 0.0769\n",
      "Epoch 89/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.7713 - accuracy: 0.0513\n",
      "Epoch 90/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.7612 - accuracy: 0.0769\n",
      "Epoch 91/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.7544 - accuracy: 0.0769\n",
      "Epoch 92/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.7504 - accuracy: 0.0769\n",
      "Epoch 93/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.7521 - accuracy: 0.0513\n",
      "Epoch 94/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.7449 - accuracy: 0.1026\n",
      "Epoch 95/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.7547 - accuracy: 0.0769\n",
      "Epoch 96/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.7307 - accuracy: 0.1282\n",
      "Epoch 97/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.7387 - accuracy: 0.0513\n",
      "Epoch 98/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.7309 - accuracy: 0.0769\n",
      "Epoch 99/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.7331 - accuracy: 0.1026\n",
      "Epoch 100/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 1.7363 - accuracy: 0.0769\n",
      "Epoch 101/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 1.7277 - accuracy: 0.1026\n",
      "Epoch 102/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 1.7200 - accuracy: 0.0769\n",
      "Epoch 103/500\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 1.7162 - accuracy: 0.1282\n",
      "Epoch 104/500\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 1.7093 - accuracy: 0.1026\n",
      "Epoch 105/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 1.7038 - accuracy: 0.0513\n",
      "Epoch 106/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.6980 - accuracy: 0.1282\n",
      "Epoch 107/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.7030 - accuracy: 0.0513\n",
      "Epoch 108/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.6942 - accuracy: 0.1026\n",
      "Epoch 109/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.7037 - accuracy: 0.1282\n",
      "Epoch 110/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 1.6890 - accuracy: 0.1282\n",
      "Epoch 111/500\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 1.6849 - accuracy: 0.1026\n",
      "Epoch 112/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.6715 - accuracy: 0.1282\n",
      "Epoch 113/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.6873 - accuracy: 0.1026\n",
      "Epoch 114/500\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 1.6721 - accuracy: 0.1282\n",
      "Epoch 115/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 1.6641 - accuracy: 0.1538\n",
      "Epoch 116/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 1.6541 - accuracy: 0.0769\n",
      "Epoch 117/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.6606 - accuracy: 0.0769\n",
      "Epoch 118/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.6646 - accuracy: 0.1282\n",
      "Epoch 119/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.6453 - accuracy: 0.1282\n",
      "Epoch 120/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.6393 - accuracy: 0.0769\n",
      "Epoch 121/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 1.6470 - accuracy: 0.0769\n",
      "Epoch 122/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 1.6348 - accuracy: 0.1282\n",
      "Epoch 123/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 1.6318 - accuracy: 0.1282\n",
      "Epoch 124/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 1.6238 - accuracy: 0.1538\n",
      "Epoch 125/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.6319 - accuracy: 0.1538\n",
      "Epoch 126/500\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 1.6325 - accuracy: 0.0769\n",
      "Epoch 127/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.6091 - accuracy: 0.1538\n",
      "Epoch 128/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.6082 - accuracy: 0.1538\n",
      "Epoch 129/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.5969 - accuracy: 0.1538\n",
      "Epoch 130/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 1.6058 - accuracy: 0.1538\n",
      "Epoch 131/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.5976 - accuracy: 0.1282\n",
      "Epoch 132/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 1.5943 - accuracy: 0.1282\n",
      "Epoch 133/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.5860 - accuracy: 0.1795\n",
      "Epoch 134/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 1.5800 - accuracy: 0.1282\n",
      "Epoch 135/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.5721 - accuracy: 0.1538\n",
      "Epoch 136/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 1.5662 - accuracy: 0.1795\n",
      "Epoch 137/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.5778 - accuracy: 0.1538\n",
      "Epoch 138/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.5644 - accuracy: 0.1538\n",
      "Epoch 139/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.5656 - accuracy: 0.1282\n",
      "Epoch 140/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.5524 - accuracy: 0.1538\n",
      "Epoch 141/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.5358 - accuracy: 0.1795\n",
      "Epoch 142/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.5453 - accuracy: 0.1538\n",
      "Epoch 143/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 1.5344 - accuracy: 0.1538\n",
      "Epoch 144/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.5264 - accuracy: 0.1282\n",
      "Epoch 145/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.5491 - accuracy: 0.1538\n",
      "Epoch 146/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 96ms/step - loss: 1.5229 - accuracy: 0.1795\n",
      "Epoch 147/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.5122 - accuracy: 0.1795\n",
      "Epoch 148/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 1.5147 - accuracy: 0.1795\n",
      "Epoch 149/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 1.5074 - accuracy: 0.1795\n",
      "Epoch 150/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 1.5044 - accuracy: 0.1795\n",
      "Epoch 151/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.4984 - accuracy: 0.1795\n",
      "Epoch 152/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.5027 - accuracy: 0.1795\n",
      "Epoch 153/500\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 1.4889 - accuracy: 0.1795\n",
      "Epoch 154/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.4793 - accuracy: 0.1795\n",
      "Epoch 155/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.4787 - accuracy: 0.1538\n",
      "Epoch 156/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 1.4747 - accuracy: 0.1795\n",
      "Epoch 157/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 1.4621 - accuracy: 0.1538\n",
      "Epoch 158/500\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 1.4552 - accuracy: 0.1795\n",
      "Epoch 159/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 1.4572 - accuracy: 0.1538\n",
      "Epoch 160/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.4469 - accuracy: 0.2051\n",
      "Epoch 161/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 1.4449 - accuracy: 0.1795\n",
      "Epoch 162/500\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 1.4456 - accuracy: 0.1795\n",
      "Epoch 163/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 1.4314 - accuracy: 0.2051\n",
      "Epoch 164/500\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 1.4365 - accuracy: 0.1538\n",
      "Epoch 165/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 1.4192 - accuracy: 0.1538\n",
      "Epoch 166/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 1.4161 - accuracy: 0.2051\n",
      "Epoch 167/500\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 1.4025 - accuracy: 0.2051\n",
      "Epoch 168/500\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 1.4026 - accuracy: 0.2051\n",
      "Epoch 169/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.3866 - accuracy: 0.2308\n",
      "Epoch 170/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.4025 - accuracy: 0.2308\n",
      "Epoch 171/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.3838 - accuracy: 0.2051\n",
      "Epoch 172/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.3779 - accuracy: 0.2051\n",
      "Epoch 173/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.3766 - accuracy: 0.2051\n",
      "Epoch 174/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.3709 - accuracy: 0.2051\n",
      "Epoch 175/500\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 1.3703 - accuracy: 0.2051\n",
      "Epoch 176/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 1.3621 - accuracy: 0.2051\n",
      "Epoch 177/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 1.3674 - accuracy: 0.1795\n",
      "Epoch 178/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 1.3473 - accuracy: 0.1795\n",
      "Epoch 179/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.3349 - accuracy: 0.2051\n",
      "Epoch 180/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.3307 - accuracy: 0.2051\n",
      "Epoch 181/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.3293 - accuracy: 0.1795\n",
      "Epoch 182/500\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 1.3279 - accuracy: 0.2051\n",
      "Epoch 183/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.3231 - accuracy: 0.2051\n",
      "Epoch 184/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 1.3283 - accuracy: 0.2308\n",
      "Epoch 185/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 1.3197 - accuracy: 0.2308\n",
      "Epoch 186/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.3113 - accuracy: 0.2308\n",
      "Epoch 187/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.2961 - accuracy: 0.2308\n",
      "Epoch 188/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 1.2912 - accuracy: 0.2051\n",
      "Epoch 189/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 1.2651 - accuracy: 0.2308\n",
      "Epoch 190/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1.2682 - accuracy: 0.2308\n",
      "Epoch 191/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.2695 - accuracy: 0.2051\n",
      "Epoch 192/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 1.2570 - accuracy: 0.2308\n",
      "Epoch 193/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 1.2625 - accuracy: 0.2308\n",
      "Epoch 194/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.2701 - accuracy: 0.2051\n",
      "Epoch 195/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 1.2560 - accuracy: 0.2051\n",
      "Epoch 196/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 1.2500 - accuracy: 0.1795\n",
      "Epoch 197/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.2423 - accuracy: 0.2308\n",
      "Epoch 198/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.2347 - accuracy: 0.2308\n",
      "Epoch 199/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 1.2250 - accuracy: 0.2308\n",
      "Epoch 200/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 1.2252 - accuracy: 0.2308\n",
      "Epoch 201/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.2105 - accuracy: 0.2308\n",
      "Epoch 202/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 1.2115 - accuracy: 0.2308\n",
      "Epoch 203/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.2056 - accuracy: 0.2308\n",
      "Epoch 204/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 1.1882 - accuracy: 0.2308\n",
      "Epoch 205/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.1738 - accuracy: 0.2308\n",
      "Epoch 206/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.1888 - accuracy: 0.2308\n",
      "Epoch 207/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.1708 - accuracy: 0.2308\n",
      "Epoch 208/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 1.1773 - accuracy: 0.2308\n",
      "Epoch 209/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.1661 - accuracy: 0.2308\n",
      "Epoch 210/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.1634 - accuracy: 0.2308\n",
      "Epoch 211/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.1531 - accuracy: 0.2308\n",
      "Epoch 212/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 1.1305 - accuracy: 0.2308\n",
      "Epoch 213/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.1450 - accuracy: 0.2308\n",
      "Epoch 214/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 1.1288 - accuracy: 0.2308\n",
      "Epoch 215/500\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 1.1381 - accuracy: 0.2308\n",
      "Epoch 216/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 1.1052 - accuracy: 0.2308\n",
      "Epoch 217/500\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 1.1280 - accuracy: 0.2308\n",
      "Epoch 218/500\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 1.1028 - accuracy: 0.2308\n",
      "Epoch 219/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.0985 - accuracy: 0.2308\n",
      "Epoch 220/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.0962 - accuracy: 0.2308\n",
      "Epoch 221/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.0756 - accuracy: 0.2308\n",
      "Epoch 222/500\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 1.0842 - accuracy: 0.2308\n",
      "Epoch 223/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 1.0723 - accuracy: 0.2308\n",
      "Epoch 224/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.0735 - accuracy: 0.2308\n",
      "Epoch 225/500\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 1.0534 - accuracy: 0.2308\n",
      "Epoch 226/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 1.0555 - accuracy: 0.2308\n",
      "Epoch 227/500\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 1.0578 - accuracy: 0.2308\n",
      "Epoch 228/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 92ms/step - loss: 1.0447 - accuracy: 0.2308\n",
      "Epoch 229/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.0353 - accuracy: 0.2308\n",
      "Epoch 230/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.0280 - accuracy: 0.2308\n",
      "Epoch 231/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.0353 - accuracy: 0.2308\n",
      "Epoch 232/500\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 1.0153 - accuracy: 0.2308\n",
      "Epoch 233/500\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 1.0025 - accuracy: 0.2308\n",
      "Epoch 234/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.9998 - accuracy: 0.2308\n",
      "Epoch 235/500\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 1.0047 - accuracy: 0.2308\n",
      "Epoch 236/500\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.9755 - accuracy: 0.2308\n",
      "Epoch 237/500\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.9855 - accuracy: 0.2308\n",
      "Epoch 238/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.9780 - accuracy: 0.2308\n",
      "Epoch 239/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.9804 - accuracy: 0.2308\n",
      "Epoch 240/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.9662 - accuracy: 0.2308\n",
      "Epoch 241/500\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.9711 - accuracy: 0.2308\n",
      "Epoch 242/500\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.9431 - accuracy: 0.2308\n",
      "Epoch 243/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.9580 - accuracy: 0.2308\n",
      "Epoch 244/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.9397 - accuracy: 0.2308\n",
      "Epoch 245/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.9343 - accuracy: 0.2308\n",
      "Epoch 246/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.9358 - accuracy: 0.2308\n",
      "Epoch 247/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.9149 - accuracy: 0.2308\n",
      "Epoch 248/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.9127 - accuracy: 0.2308\n",
      "Epoch 249/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.9103 - accuracy: 0.2308\n",
      "Epoch 250/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.9050 - accuracy: 0.2308\n",
      "Epoch 251/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.8970 - accuracy: 0.2308\n",
      "Epoch 252/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.8883 - accuracy: 0.2308\n",
      "Epoch 253/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.8765 - accuracy: 0.2308\n",
      "Epoch 254/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.8688 - accuracy: 0.2308\n",
      "Epoch 255/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.8673 - accuracy: 0.2308\n",
      "Epoch 256/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.8572 - accuracy: 0.2308\n",
      "Epoch 257/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.8581 - accuracy: 0.2308\n",
      "Epoch 258/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.8417 - accuracy: 0.2308\n",
      "Epoch 259/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.8457 - accuracy: 0.2308\n",
      "Epoch 260/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.8339 - accuracy: 0.2308\n",
      "Epoch 261/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.8309 - accuracy: 0.2308\n",
      "Epoch 262/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.8255 - accuracy: 0.2308\n",
      "Epoch 263/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.8268 - accuracy: 0.2308\n",
      "Epoch 264/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.8021 - accuracy: 0.2308\n",
      "Epoch 265/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.7994 - accuracy: 0.2308\n",
      "Epoch 266/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.7974 - accuracy: 0.2308\n",
      "Epoch 267/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.7865 - accuracy: 0.2308\n",
      "Epoch 268/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.7792 - accuracy: 0.2308\n",
      "Epoch 269/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.7766 - accuracy: 0.2308\n",
      "Epoch 270/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.7797 - accuracy: 0.2308\n",
      "Epoch 271/500\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.7626 - accuracy: 0.2308\n",
      "Epoch 272/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.7680 - accuracy: 0.2308\n",
      "Epoch 273/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.7510 - accuracy: 0.2308\n",
      "Epoch 274/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.7392 - accuracy: 0.2308\n",
      "Epoch 275/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.7290 - accuracy: 0.2308\n",
      "Epoch 276/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.7282 - accuracy: 0.2308\n",
      "Epoch 277/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.7210 - accuracy: 0.2308\n",
      "Epoch 278/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.7165 - accuracy: 0.2308\n",
      "Epoch 279/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.7039 - accuracy: 0.2308\n",
      "Epoch 280/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.7037 - accuracy: 0.2308\n",
      "Epoch 281/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6990 - accuracy: 0.2308\n",
      "Epoch 282/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6835 - accuracy: 0.2308\n",
      "Epoch 283/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.6789 - accuracy: 0.2308\n",
      "Epoch 284/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6945 - accuracy: 0.2308\n",
      "Epoch 285/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.6661 - accuracy: 0.2308\n",
      "Epoch 286/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.6647 - accuracy: 0.2308\n",
      "Epoch 287/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.6597 - accuracy: 0.2308\n",
      "Epoch 288/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.6489 - accuracy: 0.2308\n",
      "Epoch 289/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.6496 - accuracy: 0.2308\n",
      "Epoch 290/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.6461 - accuracy: 0.2308\n",
      "Epoch 291/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6347 - accuracy: 0.2308\n",
      "Epoch 292/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.6177 - accuracy: 0.2308\n",
      "Epoch 293/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6089 - accuracy: 0.2308\n",
      "Epoch 294/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.6138 - accuracy: 0.2308\n",
      "Epoch 295/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6171 - accuracy: 0.2308\n",
      "Epoch 296/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5937 - accuracy: 0.2308\n",
      "Epoch 297/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5870 - accuracy: 0.2308\n",
      "Epoch 298/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.5801 - accuracy: 0.2308\n",
      "Epoch 299/500\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.5757 - accuracy: 0.2308\n",
      "Epoch 300/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.5607 - accuracy: 0.2308\n",
      "Epoch 301/500\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.5596 - accuracy: 0.2308\n",
      "Epoch 302/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.5597 - accuracy: 0.2308\n",
      "Epoch 303/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.5457 - accuracy: 0.2308\n",
      "Epoch 304/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.5415 - accuracy: 0.2308\n",
      "Epoch 305/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.5368 - accuracy: 0.2308\n",
      "Epoch 306/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.5359 - accuracy: 0.2308\n",
      "Epoch 307/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.5202 - accuracy: 0.2308\n",
      "Epoch 308/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.5358 - accuracy: 0.2308\n",
      "Epoch 309/500\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.5186 - accuracy: 0.2308\n",
      "Epoch 310/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 92ms/step - loss: 0.5073 - accuracy: 0.2308\n",
      "Epoch 311/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4996 - accuracy: 0.2308\n",
      "Epoch 312/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.4977 - accuracy: 0.2308\n",
      "Epoch 313/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.4853 - accuracy: 0.2308\n",
      "Epoch 314/500\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.4800 - accuracy: 0.2308\n",
      "Epoch 315/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.4700 - accuracy: 0.2308\n",
      "Epoch 316/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4690 - accuracy: 0.2308\n",
      "Epoch 317/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.4575 - accuracy: 0.2308\n",
      "Epoch 318/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.4532 - accuracy: 0.2308\n",
      "Epoch 319/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.4506 - accuracy: 0.2308\n",
      "Epoch 320/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4467 - accuracy: 0.2308\n",
      "Epoch 321/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4343 - accuracy: 0.2308\n",
      "Epoch 322/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4259 - accuracy: 0.2308\n",
      "Epoch 323/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.4235 - accuracy: 0.2308\n",
      "Epoch 324/500\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.4183 - accuracy: 0.2308\n",
      "Epoch 325/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.4140 - accuracy: 0.2308\n",
      "Epoch 326/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4112 - accuracy: 0.2308\n",
      "Epoch 327/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4034 - accuracy: 0.2308\n",
      "Epoch 328/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3965 - accuracy: 0.2308\n",
      "Epoch 329/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3943 - accuracy: 0.2308\n",
      "Epoch 330/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3828 - accuracy: 0.2308\n",
      "Epoch 331/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.3833 - accuracy: 0.2308\n",
      "Epoch 332/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3674 - accuracy: 0.2308\n",
      "Epoch 333/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.3679 - accuracy: 0.2308\n",
      "Epoch 334/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.3634 - accuracy: 0.2308\n",
      "Epoch 335/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.3548 - accuracy: 0.2308\n",
      "Epoch 336/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.3505 - accuracy: 0.2308\n",
      "Epoch 337/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.3383 - accuracy: 0.2308\n",
      "Epoch 338/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.3431 - accuracy: 0.2308\n",
      "Epoch 339/500\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.3316 - accuracy: 0.2308\n",
      "Epoch 340/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.3244 - accuracy: 0.2308\n",
      "Epoch 341/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.3240 - accuracy: 0.2308\n",
      "Epoch 342/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3147 - accuracy: 0.2308\n",
      "Epoch 343/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3152 - accuracy: 0.2308\n",
      "Epoch 344/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.3054 - accuracy: 0.2308\n",
      "Epoch 345/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.3077 - accuracy: 0.2308\n",
      "Epoch 346/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.3023 - accuracy: 0.2308\n",
      "Epoch 347/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.2881 - accuracy: 0.2308\n",
      "Epoch 348/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.2885 - accuracy: 0.2308\n",
      "Epoch 349/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.2894 - accuracy: 0.2308\n",
      "Epoch 350/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.2802 - accuracy: 0.2308\n",
      "Epoch 351/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.2722 - accuracy: 0.2308\n",
      "Epoch 352/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.2703 - accuracy: 0.2308\n",
      "Epoch 353/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.2657 - accuracy: 0.2308\n",
      "Epoch 354/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.2598 - accuracy: 0.2308\n",
      "Epoch 355/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.2627 - accuracy: 0.2308\n",
      "Epoch 356/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.2520 - accuracy: 0.2308\n",
      "Epoch 357/500\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.2466 - accuracy: 0.2308\n",
      "Epoch 358/500\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.2404 - accuracy: 0.2308\n",
      "Epoch 359/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.2373 - accuracy: 0.2308\n",
      "Epoch 360/500\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.2316 - accuracy: 0.2308\n",
      "Epoch 361/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.2267 - accuracy: 0.2308\n",
      "Epoch 362/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.2275 - accuracy: 0.2308\n",
      "Epoch 363/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.2222 - accuracy: 0.2308\n",
      "Epoch 364/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.2156 - accuracy: 0.2308\n",
      "Epoch 365/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.2195 - accuracy: 0.2308\n",
      "Epoch 366/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.2122 - accuracy: 0.2308\n",
      "Epoch 367/500\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.2050 - accuracy: 0.2308\n",
      "Epoch 368/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.2060 - accuracy: 0.2308\n",
      "Epoch 369/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.2025 - accuracy: 0.2308\n",
      "Epoch 370/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.1976 - accuracy: 0.2308\n",
      "Epoch 371/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.1971 - accuracy: 0.2308\n",
      "Epoch 372/500\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.1923 - accuracy: 0.2308\n",
      "Epoch 373/500\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.1891 - accuracy: 0.2308\n",
      "Epoch 374/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.1837 - accuracy: 0.2308\n",
      "Epoch 375/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.1810 - accuracy: 0.2308\n",
      "Epoch 376/500\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.1804 - accuracy: 0.2308\n",
      "Epoch 377/500\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.1761 - accuracy: 0.2308\n",
      "Epoch 378/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.1715 - accuracy: 0.2308\n",
      "Epoch 379/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.1685 - accuracy: 0.2308\n",
      "Epoch 380/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.1636 - accuracy: 0.2308\n",
      "Epoch 381/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.1587 - accuracy: 0.2308\n",
      "Epoch 382/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.1582 - accuracy: 0.2308\n",
      "Epoch 383/500\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.1589 - accuracy: 0.2308\n",
      "Epoch 384/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.1513 - accuracy: 0.2308\n",
      "Epoch 385/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.1473 - accuracy: 0.2308\n",
      "Epoch 386/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.1509 - accuracy: 0.2308\n",
      "Epoch 387/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.1463 - accuracy: 0.2308\n",
      "Epoch 388/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.1430 - accuracy: 0.2308\n",
      "Epoch 389/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.1410 - accuracy: 0.2308\n",
      "Epoch 390/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.1358 - accuracy: 0.2308\n",
      "Epoch 391/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.1351 - accuracy: 0.2308\n",
      "Epoch 392/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 84ms/step - loss: 0.1341 - accuracy: 0.2308\n",
      "Epoch 393/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.1306 - accuracy: 0.2308\n",
      "Epoch 394/500\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.1301 - accuracy: 0.2308\n",
      "Epoch 395/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.1253 - accuracy: 0.2308\n",
      "Epoch 396/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.1249 - accuracy: 0.2308\n",
      "Epoch 397/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.1256 - accuracy: 0.2308\n",
      "Epoch 398/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.1205 - accuracy: 0.2308\n",
      "Epoch 399/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.1185 - accuracy: 0.2308\n",
      "Epoch 400/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.1153 - accuracy: 0.2308\n",
      "Epoch 401/500\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.1148 - accuracy: 0.2308\n",
      "Epoch 402/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.1131 - accuracy: 0.2308\n",
      "Epoch 403/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.1092 - accuracy: 0.2308\n",
      "Epoch 404/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.1108 - accuracy: 0.2308\n",
      "Epoch 405/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.1114 - accuracy: 0.2308\n",
      "Epoch 406/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.1063 - accuracy: 0.2308\n",
      "Epoch 407/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.1018 - accuracy: 0.2308\n",
      "Epoch 408/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.1020 - accuracy: 0.2308\n",
      "Epoch 409/500\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0979 - accuracy: 0.2308\n",
      "Epoch 410/500\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0993 - accuracy: 0.2308\n",
      "Epoch 411/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0980 - accuracy: 0.2308\n",
      "Epoch 412/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0948 - accuracy: 0.2308\n",
      "Epoch 413/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0944 - accuracy: 0.2308\n",
      "Epoch 414/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0919 - accuracy: 0.2308\n",
      "Epoch 415/500\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0908 - accuracy: 0.2308\n",
      "Epoch 416/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0884 - accuracy: 0.2308\n",
      "Epoch 417/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0873 - accuracy: 0.2308\n",
      "Epoch 418/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0877 - accuracy: 0.2308\n",
      "Epoch 419/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0841 - accuracy: 0.2308\n",
      "Epoch 420/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0841 - accuracy: 0.2308\n",
      "Epoch 421/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0802 - accuracy: 0.2308\n",
      "Epoch 422/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0812 - accuracy: 0.2308\n",
      "Epoch 423/500\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0773 - accuracy: 0.2308\n",
      "Epoch 424/500\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0787 - accuracy: 0.2308\n",
      "Epoch 425/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0787 - accuracy: 0.2308\n",
      "Epoch 426/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0740 - accuracy: 0.2308\n",
      "Epoch 427/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0740 - accuracy: 0.2308\n",
      "Epoch 428/500\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0735 - accuracy: 0.2308\n",
      "Epoch 429/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0702 - accuracy: 0.2308\n",
      "Epoch 430/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0709 - accuracy: 0.2308\n",
      "Epoch 431/500\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0677 - accuracy: 0.2308\n",
      "Epoch 432/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0678 - accuracy: 0.2308\n",
      "Epoch 433/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0677 - accuracy: 0.2308\n",
      "Epoch 434/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0653 - accuracy: 0.2308\n",
      "Epoch 435/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0634 - accuracy: 0.2308\n",
      "Epoch 436/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0622 - accuracy: 0.2308\n",
      "Epoch 437/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0630 - accuracy: 0.2308\n",
      "Epoch 438/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0617 - accuracy: 0.2308\n",
      "Epoch 439/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0621 - accuracy: 0.2308\n",
      "Epoch 440/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0607 - accuracy: 0.2308\n",
      "Epoch 441/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0600 - accuracy: 0.2308\n",
      "Epoch 442/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0592 - accuracy: 0.2308\n",
      "Epoch 443/500\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0570 - accuracy: 0.2308\n",
      "Epoch 444/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0564 - accuracy: 0.2308\n",
      "Epoch 445/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0560 - accuracy: 0.2308\n",
      "Epoch 446/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0526 - accuracy: 0.2308\n",
      "Epoch 447/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0530 - accuracy: 0.2308\n",
      "Epoch 448/500\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0521 - accuracy: 0.2308\n",
      "Epoch 449/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0551 - accuracy: 0.2308\n",
      "Epoch 450/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0501 - accuracy: 0.2308\n",
      "Epoch 451/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0506 - accuracy: 0.2308\n",
      "Epoch 452/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0512 - accuracy: 0.2308\n",
      "Epoch 453/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0501 - accuracy: 0.2308\n",
      "Epoch 454/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0483 - accuracy: 0.2308\n",
      "Epoch 455/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0484 - accuracy: 0.2308\n",
      "Epoch 456/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0474 - accuracy: 0.2308\n",
      "Epoch 457/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0470 - accuracy: 0.2308\n",
      "Epoch 458/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0448 - accuracy: 0.2308\n",
      "Epoch 459/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0442 - accuracy: 0.2308\n",
      "Epoch 460/500\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0443 - accuracy: 0.2308\n",
      "Epoch 461/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0434 - accuracy: 0.2308\n",
      "Epoch 462/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0436 - accuracy: 0.2308\n",
      "Epoch 463/500\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0442 - accuracy: 0.2308\n",
      "Epoch 464/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0417 - accuracy: 0.2308\n",
      "Epoch 465/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0397 - accuracy: 0.2308\n",
      "Epoch 466/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0405 - accuracy: 0.2308\n",
      "Epoch 467/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0394 - accuracy: 0.2308\n",
      "Epoch 468/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0397 - accuracy: 0.2308\n",
      "Epoch 469/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0392 - accuracy: 0.2308\n",
      "Epoch 470/500\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0383 - accuracy: 0.2308\n",
      "Epoch 471/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0377 - accuracy: 0.2308\n",
      "Epoch 472/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0366 - accuracy: 0.2308\n",
      "Epoch 473/500\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0351 - accuracy: 0.2308\n",
      "Epoch 474/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0365 - accuracy: 0.2308\n",
      "Epoch 475/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0344 - accuracy: 0.2308\n",
      "Epoch 476/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0345 - accuracy: 0.2308\n",
      "Epoch 477/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0350 - accuracy: 0.2308\n",
      "Epoch 478/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0342 - accuracy: 0.2308\n",
      "Epoch 479/500\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0328 - accuracy: 0.2308\n",
      "Epoch 480/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0335 - accuracy: 0.2308\n",
      "Epoch 481/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0332 - accuracy: 0.2308\n",
      "Epoch 482/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0322 - accuracy: 0.2308\n",
      "Epoch 483/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0306 - accuracy: 0.2308\n",
      "Epoch 484/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0320 - accuracy: 0.2308\n",
      "Epoch 485/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0296 - accuracy: 0.2308\n",
      "Epoch 486/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0298 - accuracy: 0.2308\n",
      "Epoch 487/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0294 - accuracy: 0.2308\n",
      "Epoch 488/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0292 - accuracy: 0.2308\n",
      "Epoch 489/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0290 - accuracy: 0.2308\n",
      "Epoch 490/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0297 - accuracy: 0.2308\n",
      "Epoch 491/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0275 - accuracy: 0.2308\n",
      "Epoch 492/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0277 - accuracy: 0.2308\n",
      "Epoch 493/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0271 - accuracy: 0.2308\n",
      "Epoch 494/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0266 - accuracy: 0.2308\n",
      "Epoch 495/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0265 - accuracy: 0.2308\n",
      "Epoch 496/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0254 - accuracy: 0.2308\n",
      "Epoch 497/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0250 - accuracy: 0.2308\n",
      "Epoch 498/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0250 - accuracy: 0.2308\n",
      "Epoch 499/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0258 - accuracy: 0.2308\n",
      "Epoch 500/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0243 - accuracy: 0.2308\n",
      "Input: 아 피곤해...\n",
      "Output: 그렇다면 당신의 역할은 다 한거예요.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-77ffcd6e5fcf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"아 피곤해...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 502\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"안녕하세요\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\deeplearn\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deeplearn\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    904\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    905\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_v2_behavior\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 906\u001b[1;33m           \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dims\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    907\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    908\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dims\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import time\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "batch_size=64\n",
    "buffer_size=20000\n",
    "\"\"\"\n",
    "df1=pd.read_excel('1.xlsx')\n",
    "df2=pd.read_excel('3.xlsx')\n",
    "df3=pd.read_excel('6.xlsx')\n",
    "df4=pd.read_excel('11.xlsx')\n",
    "df5=pd.read_excel('12.xlsx')\n",
    "df6=pd.read_excel('13.xlsx')\n",
    "df7=pd.read_excel(\"qa1.xlsx\")\n",
    "# print(df4.columns)\n",
    "# print(df5.columns)\n",
    "# print(df6.columns)\n",
    "df_medium1=pd.concat([df4,df5,df6])\n",
    "# print(df_medium1.info())\n",
    "df_medium2=pd.concat([df1,df2,df3])\n",
    "print(df_medium2.info())\n",
    "df_medium1.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
    "df_medium2.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
    "df_medium1.rename(columns={'question':'Q','answer':'A'},inplace=True)\n",
    "df_medium2.rename(columns={'MQ':'Q','SA':'A'},inplace=True)\n",
    "df_medium3=pd.concat([df_medium1,df_medium2])\n",
    "print(df_medium3.info())\n",
    "df_final=pd.concat([df_medium3,df7])\n",
    "df_final.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
    "\"\"\"\n",
    "df=pd.read_excel('comunications.xlsx')\n",
    "df2=pd.read_csv('ChatbotData.csv')\n",
    "df3=pd.read_excel('웰니스_대화_스크립트_데이터셋.xlsx')\n",
    "df2.drop(['label'], axis=1,inplace=True)\n",
    "df3.drop(['구분'],axis=1, inplace=True)\n",
    "df3.rename(columns = {'유저' : 'Q','챗봇':'A'}, inplace = True)\n",
    "\n",
    "df1=pd.concat([df,df2])\n",
    "\n",
    "df_final=pd.concat([df1,df3])\n",
    "df_final.dropna(axis=0, inplace=True)\n",
    "print(df_final.info())\n",
    "\n",
    "df_final=df_final.sample(frac=1).reset_index(drop=True)\n",
    "print(df_final)\n",
    "question=[]\n",
    "answers=[]\n",
    "for sentence in df_final['Q']:\n",
    "    sentence=re.sub(r\"([?.!,])\",r\"\\1 \",sentence)\n",
    "    sentence=sentence.strip()\n",
    "    question.append(sentence)\n",
    "for sentence in df_final['A']:\n",
    "    sentence=re.sub(r\"([?.!,])\",r\"\\1 \",sentence)\n",
    "    sentence=sentence.strip()\n",
    "    answers.append(sentence)\n",
    "\n",
    "    \n",
    "# 정수 인코딩 & 단어 사전 형성\n",
    "tokenizer=tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(question+answers, target_vocab_size=2**13)\n",
    "START_TOKEN, END_TOKEN =[tokenizer.vocab_size],[tokenizer.vocab_size+1]\n",
    "VOCAB_SIZE=tokenizer.vocab_size+2\n",
    "print('임의 질문 샘플 정수 인코딩: {}'.format(tokenizer.encode(question[20])))\n",
    "\n",
    "# 문장의 최대 정수를 정해줌\n",
    "MAX_LENGTH=40\n",
    "def tokenize_and_filter(inputs,outputs):\n",
    "    tokenized_inputs, tokenized_outputs=[],[]\n",
    "    for(sentence1, sentence2) in zip(inputs,outputs):\n",
    "         # 인 코딩 시작, 종료 토큰 추가 \n",
    "        sentence1=START_TOKEN+tokenizer.encode(sentence1)+END_TOKEN\n",
    "        sentence2=END_TOKEN+tokenizer.encode(sentence2)+END_TOKEN\n",
    "        \n",
    "        tokenized_inputs.append(sentence1)\n",
    "        tokenized_outputs.append(sentence2)\n",
    "        \n",
    "        # 패딩\n",
    "        tokenized_inputs=tf.keras.preprocessing.sequence.pad_sequences(tokenized_inputs, maxlen=MAX_LENGTH,padding='post')\n",
    "        tokenized_outputs=tf.keras.preprocessing.sequence.pad_sequences(tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "        \n",
    "        return tokenized_inputs, tokenized_outputs\n",
    "    \n",
    "question,answers=tokenize_and_filter(question, answers)\n",
    "print(question[0])\n",
    "\n",
    "dataset=tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs':question,\n",
    "        'dec_inputs':answers[:,:-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs':answers[:,1:]\n",
    "    },\n",
    "\n",
    "))\n",
    "\n",
    "dataset=dataset.cache()\n",
    "dataset=dataset.shuffle(buffer_size)\n",
    "dataset=dataset.batch(batch_size)\n",
    "dataset=dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# 트랜스 포머 모델\n",
    "def transformer(vocab_size, num_layers, dff,\n",
    "                d_model, num_heads, dropout,\n",
    "                name=\"transformer\"):\n",
    "\n",
    "  # 인코더의 입력\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "  # 디코더의 입력\n",
    "  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "  # 인코더의 패딩 마스크\n",
    "  enc_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='enc_padding_mask')(inputs)\n",
    "\n",
    "  # 디코더의 룩어헤드 마스크(첫번째 서브층)\n",
    "  look_ahead_mask = tf.keras.layers.Lambda(\n",
    "      create_look_ahead_mask, output_shape=(1, None, None),\n",
    "      name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "  # 디코더의 패딩 마스크(두번째 서브층)\n",
    "  dec_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='dec_padding_mask')(inputs)\n",
    "\n",
    "  # 인코더의 출력은 enc_outputs. 디코더로 전달된다.\n",
    "  enc_outputs = encoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,\n",
    "      d_model=d_model, num_heads=num_heads, dropout=dropout,\n",
    "  )(inputs=[inputs, enc_padding_mask]) # 인코더의 입력은 입력 문장과 패딩 마스크\n",
    "\n",
    "  # 디코더의 출력은 dec_outputs. 출력층으로 전달된다.\n",
    "  dec_outputs = decoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,\n",
    "      d_model=d_model, num_heads=num_heads, dropout=dropout,\n",
    "  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "  # 다음 단어 예측을 위한 출력층\n",
    "  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding=self.positional_encoding(position, d_model)\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles=1/tf.pow(10000,(2*(i//2))/tf.cast(d_model,tf.float32))\n",
    "        return position * angles\n",
    "    def positional_encoding(self,position, d_model):\n",
    "        angle_rads=self.get_angles(position=tf.range(position, dtype=tf.float32)[:,tf.newaxis],i=tf.range(d_model, dtype=tf.float32)[tf.newaxis,:],d_model=d_model)\n",
    "        # 짝수에는 사인 함수 적용\n",
    "        sines=tf.math.sin(angle_rads[:,0::2])\n",
    "        # 홀수에는 코사인 함수 적용\n",
    "        cosines=tf.math.cos(angle_rads[:,1::2])\n",
    "        \n",
    "        angle_rads=np.zeros(angle_rads.shape)\n",
    "        angle_rads[:,0::2]=sines\n",
    "        angle_rads[:,1::2]=cosines\n",
    "        \n",
    "        pos_encoding=tf.constant(angle_rads)\n",
    "        pos_encoding=pos_encoding[tf.newaxis,...]\n",
    "        print(pos_encoding.shape)\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "    def call(self, inputs):\n",
    "        return inputs+self.pos_encoding[:,:tf.shape(inputs)[1],:]\n",
    " \n",
    "def create_padding_mask(x):\n",
    "    mask=tf.cast(tf.math.equal(x,0),tf.float32)\n",
    "    return mask[:,tf.newaxis,tf.newaxis,:]\n",
    "\n",
    "# 디코더의 첫번째 서브층(sublayer)에서 미래 토큰을 Mask하는 함수\n",
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x) # 패딩 마스크도 포함\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)\n",
    "\n",
    "#encoder\n",
    "def encoder(vocab_size, num_layers, dff,\n",
    "                d_model, num_heads, dropout,\n",
    "                name=\"encoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "  # 인코더는 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # 포지셔널 인코딩 + 드롭아웃\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  # 인코더를 num_layers개 쌓기\n",
    "    for i in range(num_layers):\n",
    "        outputs = encoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\n",
    "            dropout=dropout, name=\"encoder_layer_{}\".format(i),\n",
    "        )([outputs, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, padding_mask], outputs=outputs, name=name)     \n",
    "\n",
    "def encoder_layer(dff, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "      # 인코더는 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "      # 멀티-헤드 어텐션 (첫번째 서브층 / 셀프 어텐션)\n",
    "    attention = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention\")({\n",
    "            'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V\n",
    "            'mask': padding_mask # 패딩 마스크 사용\n",
    "            })\n",
    "\n",
    "      # 드롭아웃 + 잔차 연결과 층 정규화\n",
    "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "    attention = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "      # 포지션 와이즈 피드 포워드 신경망 (두번째 서브층)\n",
    "    outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "      # 드롭아웃 + 잔차 연결과 층 정규화\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "    # d_model을 num_heads로 나눈 값.\n",
    "    # 논문 기준 : 64\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "    # WQ, WK, WV에 해당하는 밀집층 정의\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    # WO에 해당하는 밀집층 정의\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "  # num_heads 개수만큼 q, k, v를 split하는 함수\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(\n",
    "            inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "        'value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "    # 1. WQ, WK, WV에 해당하는 밀집층 지나기\n",
    "    # q : (batch_size, query의 문장 길이, d_model)\n",
    "    # k : (batch_size, key의 문장 길이, d_model)\n",
    "    # v : (batch_size, value의 문장 길이, d_model)\n",
    "    # 참고) 인코더(k, v)-디코더(q) 어텐션에서는 query 길이와 key, value의 길이는 다를 수 있다.\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "    # 2. 헤드 나누기\n",
    "    # q : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "    # k : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n",
    "    # v : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "    # 3. 스케일드 닷 프로덕트 어텐션. 앞서 구현한 함수 사용.\n",
    "    # (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "        scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask)\n",
    "    # (batch_size, query의 문장 길이, num_heads, d_model/num_heads)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "    # 4. 헤드 연결(concatenate)하기\n",
    "    # (batch_size, query의 문장 길이, d_model)\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))\n",
    "\n",
    "    # 5. WO에 해당하는 밀집층 지나기\n",
    "    # (batch_size, query의 문장 길이, d_model)\n",
    "        outputs = self.dense(concat_attention)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "  # query 크기 : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "  # key 크기 : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n",
    "  # value 크기 : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n",
    "  # padding_mask : (batch_size, 1, 1, key의 문장 길이)\n",
    "\n",
    "  # Q와 K의 곱. 어텐션 스코어 행렬.\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "  # 스케일링\n",
    "  # dk의 루트값으로 나눠준다.\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "  # 마스킹. 어텐션 스코어 행렬의 마스킹 할 위치에 매우 작은 음수값을 넣는다.\n",
    "  # 매우 작은 값이므로 소프트맥스 함수를 지나면 행렬의 해당 위치의 값은 0이 된다.\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "\n",
    "  # 소프트맥스 함수는 마지막 차원인 key의 문장 길이 방향으로 수행된다.\n",
    "  # attention weight : (batch_size, num_heads, query의 문장 길이, key의 문장 길이)\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "  # output : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "def decoder(vocab_size, num_layers, dff,\n",
    "            d_model, num_heads, dropout,\n",
    "            name='decoder'):\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "\n",
    "  # 디코더는 룩어헤드 마스크(첫번째 서브층)와 패딩 마스크(두번째 서브층) 둘 다 사용.\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name='look_ahead_mask')\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "  # 포지셔널 인코딩 + 드롭아웃\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  # 디코더를 num_layers개 쌓기\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\n",
    "            dropout=dropout, name='decoder_layer_{}'.format(i),\n",
    "        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)\n",
    "\n",
    "def decoder_layer(dff, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "\n",
    "  # 룩어헤드 마스크(첫번째 서브층)\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "        shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "\n",
    "  # 패딩 마스크(두번째 서브층)\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "  # 멀티-헤드 어텐션 (첫번째 서브층 / 마스크드 셀프 어텐션)\n",
    "    attention1 = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V\n",
    "          'mask': look_ahead_mask # 룩어헤드 마스크\n",
    "        })\n",
    "\n",
    "  # 잔차 연결과 층 정규화\n",
    "    attention1 = tf.keras.layers.LayerNormalization(\n",
    "          epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "  # 멀티-헤드 어텐션 (두번째 서브층 / 디코더-인코더 어텐션)\n",
    "    attention2 = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "            'query': attention1, 'key': enc_outputs, 'value': enc_outputs, # Q != K = V\n",
    "            'mask': padding_mask # 패딩 마스크\n",
    "        })\n",
    "\n",
    "  # 드롭아웃 + 잔차 연결과 층 정규화\n",
    "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "    attention2 = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "  # 포지션 와이즈 피드 포워드 신경망 (세번째 서브층)\n",
    "    outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention2)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 드롭아웃 + 잔차 연결과 층 정규화\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "D_MODEL = 256\n",
    "NUM_LAYERS = 2\n",
    "NUM_HEADS = 8\n",
    "DFF = 512\n",
    "DROPOUT = 0.1\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dff=DFF,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "            arg1 = tf.math.rsqrt(step)\n",
    "            arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "            return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "learning_rate=CustomSchedule(D_MODEL)\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "def accuracy(y_true,y_pred):\n",
    "    y_true=tf.reshape(y_true, shape=(-1,MAX_LENGTH-1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
    "\n",
    "EPOCHS=500\n",
    "model.fit(dataset,epochs=EPOCHS)\n",
    "\n",
    "def evaluate(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "    output = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "  # 디코더의 예측 시작\n",
    "    for i in range(MAX_LENGTH):\n",
    "        predictions = model(inputs=[sentence, output], training=False)\n",
    "\n",
    "    # 현재(마지막) 시점의 예측 단어를 받아온다.\n",
    "        predictions = predictions[:, -1:, :]\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # 만약 마지막 시점의 예측 단어가 종료 토큰이라면 예측을 중단\n",
    "        if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "             break\n",
    "\n",
    "    # 마지막 시점의 예측 단어를 출력에 연결한다.\n",
    "    # 이는 for문을 통해서 디코더의 입력으로 사용될 예정이다.\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0)\n",
    "\n",
    "def predict(sentence):\n",
    "    prediction = evaluate(sentence)\n",
    "\n",
    "    predicted_sentence = tokenizer.decode(\n",
    "        [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Output: {}'.format(predicted_sentence))\n",
    "\n",
    "    return predicted_sentence\n",
    "  \n",
    "  \n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence\n",
    "\n",
    "\n",
    "output=predict(\"아 피곤해...\")\n",
    "print(model.predict(\"안녕하세요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c306bcc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a857dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356a0ceb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
